{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b2a21598",
      "metadata": {
        "id": "b2a21598"
      },
      "source": [
        "# WEEK 37\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "lprgdS0lp3C8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lprgdS0lp3C8",
        "outputId": "881d18ce-4838-45ea-bca9-bcfa3420440c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "default-jdk is already the newest version (2:1.11-72build2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q camel-tools konlpy indic-nlp-library\n",
        "!apt -y install default-jdk\n",
        "\n",
        "## !pip install --upgrade --force-reinstall konlpy camel-tools indic-nlp-library\n",
        "# if having  problems in next block\n",
        "\n",
        "#\"ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "vdaTFrTpwISh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdaTFrTpwISh",
        "outputId": "de1c160f-cea7-4807-8b79-eab40777b7d5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import math, pandas as pd, nltk\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline, pad_both_ends\n",
        "from nltk.lm import Laplace, KneserNeyInterpolated, WittenBellInterpolated\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from camel_tools.tokenizers.word import simple_word_tokenize # Arabic\n",
        "from konlpy.tag import Okt # Korean\n",
        "from indicnlp.tokenize import indic_tokenize # Telugu\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt'); nltk.download('punkt_tab')\n",
        "\n",
        "okt = Okt() # for Korean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "Esd8lJoepMxp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Esd8lJoepMxp",
        "outputId": "7b511305-a42c-4db0-c9a8-ecce5fd769da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ar n=2 laplace: PPL=34.56 CE=3.5428\n",
            "ar n=2 kn: PPL=13.23 CE=2.5825\n",
            "ar n=2 wb: PPL=12.83 CE=2.5519\n",
            "ar n=3 laplace: PPL=54.72 CE=4.0023\n",
            "ar n=3 kn: PPL=12.03 CE=2.4878\n",
            "ar n=3 wb: PPL=9.78 CE=2.2801\n",
            "ko n=2 laplace: PPL=30.41 CE=3.4148\n",
            "ko n=2 kn: PPL=11.89 CE=2.4756\n",
            "ko n=2 wb: PPL=11.51 CE=2.4429\n",
            "ko n=3 laplace: PPL=49.59 CE=3.9037\n",
            "ko n=3 kn: PPL=9.71 CE=2.2728\n",
            "ko n=3 wb: PPL=8.16 CE=2.0992\n",
            "te n=2 laplace: PPL=30.61 CE=3.4214\n",
            "te n=2 kn: PPL=10.99 CE=2.3966\n",
            "te n=2 wb: PPL=10.89 CE=2.3879\n",
            "te n=3 laplace: PPL=43.59 CE=3.7747\n",
            "te n=3 kn: PPL=8.54 CE=2.1443\n",
            "te n=3 wb: PPL=7.28 CE=1.9854\n",
            "en n=2 laplace: PPL=1508.92 CE=7.3192\n",
            "en n=2 wb: PPL=209.77 CE=5.3460\n",
            "en n=3 laplace: PPL=8069.52 CE=8.9958\n",
            "en n=3 wb: PPL=146.71 CE=4.9885\n",
            "   language  order smoothing  vocab_size  tokens  perplexity  cross_entropy\n",
            "1        ar      2        kn         965    3457       13.23         2.5825\n",
            "0        ar      2   laplace         965    3457       34.56         3.5428\n",
            "2        ar      2        wb         965    3457       12.83         2.5519\n",
            "4        ar      3        kn         965    3872       12.03         2.4878\n",
            "3        ar      3   laplace         965    3872       54.72         4.0023\n",
            "5        ar      3        wb         965    3872        9.78         2.2801\n",
            "18       en      2   laplace       29119  353455     1508.92         7.3192\n",
            "19       en      2        wb       29119  353455      209.77         5.3460\n",
            "20       en      3   laplace       29119  356466     8069.52         8.9958\n",
            "21       en      3        wb       29119  356466      146.71         4.9885\n",
            "7        ko      2        kn         755    3608       11.89         2.4756\n",
            "6        ko      2   laplace         755    3608       30.41         3.4148\n",
            "8        ko      2        wb         755    3608       11.51         2.4429\n",
            "10       ko      3        kn         755    3964        9.71         2.2728\n",
            "9        ko      3   laplace         755    3964       49.59         3.9037\n",
            "11       ko      3        wb         755    3964        8.16         2.0992\n",
            "13       te      2        kn         479    3074       10.99         2.3966\n",
            "12       te      2   laplace         479    3074       30.61         3.4214\n",
            "14       te      2        wb         479    3074       10.89         2.3879\n",
            "16       te      3        kn         479    3458        8.54         2.1443\n",
            "15       te      3   laplace         479    3458       43.59         3.7747\n",
            "17       te      3        wb         479    3458        7.28         1.9854\n"
          ]
        }
      ],
      "source": [
        "splits = {'train': 'train.parquet', 'validation': 'validation.parquet'}\n",
        "df_train = pd.read_parquet(\"hf://datasets/coastalcph/tydi_xor_rc/\" + splits[\"train\"])\n",
        "df_val = pd.read_parquet(\"hf://datasets/coastalcph/tydi_xor_rc/\" + splits[\"validation\"])\n",
        "\n",
        "QUESTION, CONTEXT, LANG = \"question\", \"context\", \"lang\"\n",
        "LANGUAGES = [\"ar\", \"ko\", \"te\", \"en\"]\n",
        "Ns = [2, 3]\n",
        "SMOOTHINGS = [\"laplace\", \"kn\", \"wb\"]\n",
        "\n",
        "# tokenizers per language\n",
        "def select_tokenizer(lang):\n",
        "    if lang == \"ar\":\n",
        "        return simple_word_tokenize\n",
        "    if lang == \"ko\":\n",
        "        return okt.morphs\n",
        "    if lang == \"te\":\n",
        "        return lambda s: list(indic_tokenize.trivial_tokenize(s))\n",
        "    if lang == \"en\":\n",
        "        return nltk.word_tokenize\n",
        "\n",
        "def tokenize_column(series, lang):\n",
        "    tokenize = select_tokenizer(lang)\n",
        "    return [tokenize(str(x).lower()) for x in series.dropna()]\n",
        "\n",
        "def build_vocab(tokenized, min_freq):\n",
        "    counter = Counter(w for s in tokenized for w in s)\n",
        "    vocab = {w:i for i,(w,c) in enumerate(counter.items()) if c >= min_freq}\n",
        "    vocab[\"<OOV>\"] = len(vocab)\n",
        "\n",
        "    return vocab\n",
        "\n",
        "# replace all tokens that are not part of the vocab with <OOV> token\n",
        "def apply_oov(tokenized, vocab):\n",
        "    return [[w if w in vocab else \"<OOV>\" for w in s] for s in tokenized]\n",
        "\n",
        "def train_lm(tokenized, n, smoothing):\n",
        "    train_grams, vocab = padded_everygram_pipeline(n, tokenized)\n",
        "    if smoothing == \"laplace\":\n",
        "      model = Laplace(n)\n",
        "    elif smoothing == \"kn\":\n",
        "      model = KneserNeyInterpolated(n)\n",
        "    elif smoothing == \"wb\":\n",
        "      model = WittenBellInterpolated(n)\n",
        "    else:\n",
        "      raise ValueError(smoothing)\n",
        "    model.fit(train_grams, vocab)\n",
        "\n",
        "    return model\n",
        "\n",
        "# calculate perplexity\n",
        "def perplexity(model, tokenized_sentences, n):\n",
        "    sum_log_probs = 0.0\n",
        "    count_ngrams = 0\n",
        "    for sentence in tokenized_sentences:\n",
        "        for ngram in ngrams(list(pad_both_ends(sentence, n=n)), n):\n",
        "            context, w = ngram[:-1], ngram[-1]\n",
        "            prob = model.score(w, context) or 1e-12\n",
        "            sum_log_probs += math.log(prob); count_ngrams += 1\n",
        "    cross_entropy = - sum_log_probs / max(count_ngrams, 1)\n",
        "\n",
        "    return math.exp(cross_entropy), cross_entropy, count_ngrams\n",
        "\n",
        "# train and evaluate\n",
        "rows = []\n",
        "for L in LANGUAGES:\n",
        "    if L == \"en\":\n",
        "        tr_txt = df_train[CONTEXT]\n",
        "        val_txt = df_val[CONTEXT]\n",
        "        smooths = [s for s in SMOOTHINGS if s != \"kn\"]  # leave out KN for english (copmutational issue)\n",
        "    else:\n",
        "        tr_txt = df_train.loc[df_train[LANG]==L, QUESTION]\n",
        "        val_txt = df_val.loc[df_val[LANG]==L, QUESTION]\n",
        "        smooths = SMOOTHINGS\n",
        "\n",
        "    tr_tok = tokenize_column(tr_txt, L)\n",
        "    val_tok = tokenize_column(val_txt, L)\n",
        "    vocab = build_vocab(tr_tok, 3)\n",
        "    tr_tok = apply_oov(tr_tok, vocab)\n",
        "    val_tok = apply_oov(val_tok, vocab)\n",
        "\n",
        "    for n in Ns:\n",
        "        for s in smooths:\n",
        "            model = train_lm(tr_tok, n, s)\n",
        "            ppl, cross_entropy, num_tokens = perplexity(model, val_tok, n)\n",
        "            rows.append(dict(language=L, order=n, smoothing=s, vocab_size=len(vocab), tokens=num_tokens, perplexity=round(ppl, 2), cross_entropy=round(cross_entropy, 4)))\n",
        "            print(f\"{L} n={n} {s}: PPL={ppl:.2f} CE={cross_entropy:.4f}\")\n",
        "\n",
        "res_df = pd.DataFrame(rows).sort_values([\"language\",\"order\",\"smoothing\"])\n",
        "print(res_df)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
