{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cfc3380d",
      "metadata": {
        "id": "cfc3380d"
      },
      "outputs": [],
      "source": [
        "##IMPORTS\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "from collections import Counter\n",
        "from transformers import pipeline\n",
        "from googletrans import Translator\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "import regex as re\n",
        "from collections import Counter\n",
        "from googletrans import Translator\n",
        "import pickle\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13178b08",
      "metadata": {
        "id": "13178b08"
      },
      "source": [
        "# WEEK 36"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a67142ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a67142ad",
        "outputId": "7c9c2ede-ae8e-4891-b28c-08a93991dc47"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#DOWNLOAD DATASET\n",
        "\n",
        "splits = {'train': 'train.parquet', 'validation': 'validation.parquet'}\n",
        "df_train = pd.read_parquet(\"hf://datasets/coastalcph/tydi_xor_rc/\" + splits[\"train\"])\n",
        "df_val = pd.read_parquet(\"hf://datasets/coastalcph/tydi_xor_rc/\" + splits[\"validation\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec0aac73",
      "metadata": {
        "id": "ec0aac73"
      },
      "source": [
        "## Stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "316f9bba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "316f9bba",
        "outputId": "2406c09c-b076-4218-a7bd-c46bf9b80af7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset sizes for selected languages:\n",
            "      train_size  val_size\n",
            "lang                      \n",
            "ar          2558       415\n",
            "ko          2422       356\n",
            "te          1355       384\n"
          ]
        }
      ],
      "source": [
        "#STATS\n",
        "\n",
        "#SIZE\n",
        "\n",
        "langs = [\"ar\", \"ko\", \"te\"]\n",
        "\n",
        "\n",
        "train_counts = df_train[df_train[\"lang\"].isin(langs)].groupby(\"lang\").size()\n",
        "\n",
        "\n",
        "val_counts = df_val[df_val[\"lang\"].isin(langs)].groupby(\"lang\").size()\n",
        "\n",
        "size_df = pd.DataFrame({\n",
        "    \"train_size\": train_counts,\n",
        "    \"val_size\": val_counts\n",
        "}).fillna(0).astype(int)\n",
        "\n",
        "print(\"Dataset sizes for selected languages:\")\n",
        "print(size_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "905939a2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "905939a2",
        "outputId": "e7a4a986-e9c9-4164-d116-6b10420b77ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Arabic — TRAIN punctuation (char -> count):\n",
            "[('؟', 2556), ('\"', 80), ('(', 25), (')', 25), ('-', 5), ('.', 2), ('/', 2), ('«', 2), ('»', 2), ('_', 2), ('\\\\', 1), ('—', 1), ('!', 1), ('،', 1)]\n",
            "Arabic — VAL punctuation (char -> count):\n",
            "[('؟', 413), ('\"', 4), ('(', 3), (')', 3), ('،', 1), ('-', 1)]\n",
            "Korean — TRAIN punctuation (char -> count):\n",
            "[('?', 2420), (',', 23), ('.', 16), (\"'\", 6), ('\"', 6), ('-', 5), (':', 2), ('/', 1), ('\\\\', 1), ('(', 1), (')', 1)]\n",
            "Korean — VAL punctuation (char -> count):\n",
            "[('?', 356), ('.', 9), (',', 3), ('-', 1)]\n",
            "Telugu — TRAIN punctuation (char -> count):\n",
            "[('?', 1355), ('.', 42), (',', 6), ('-', 3), ('%', 1), ('–', 1)]\n",
            "Telugu — VAL punctuation (char -> count):\n",
            "[('?', 384), ('.', 2), ('-', 1), ('%', 1)]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "## Each language punctuation\n",
        "PUNCT_RE = re.compile(r\"\\p{P}\", re.UNICODE)\n",
        "\n",
        "# ARABIC\n",
        "ar_train_q = df_train[df_train[\"lang\"] == \"ar\"][\"question\"].astype(str)\n",
        "ar_val_q   = df_val[df_val[\"lang\"] == \"ar\"][\"question\"].astype(str)\n",
        "\n",
        "ar_train_punct = Counter(ch for q in ar_train_q for ch in PUNCT_RE.findall(q))\n",
        "ar_val_punct   = Counter(ch for q in ar_val_q for ch in PUNCT_RE.findall(q))\n",
        "\n",
        "print(\"Arabic — TRAIN punctuation (char -> count):\")\n",
        "print(ar_train_punct.most_common())\n",
        "print(\"Arabic — VAL punctuation (char -> count):\")\n",
        "print(ar_val_punct.most_common())\n",
        "\n",
        "# KOREAN\n",
        "ko_train_q = df_train[df_train[\"lang\"] == \"ko\"][\"question\"].astype(str)\n",
        "ko_val_q   = df_val[df_val[\"lang\"] == \"ko\"][\"question\"].astype(str)\n",
        "\n",
        "ko_train_punct = Counter(ch for q in ko_train_q for ch in PUNCT_RE.findall(q))\n",
        "ko_val_punct   = Counter(ch for q in ko_val_q for ch in PUNCT_RE.findall(q))\n",
        "\n",
        "print(\"Korean — TRAIN punctuation (char -> count):\")\n",
        "print(ko_train_punct.most_common())\n",
        "print(\"Korean — VAL punctuation (char -> count):\")\n",
        "print(ko_val_punct.most_common())\n",
        "\n",
        "# TELUGU\n",
        "te_train_q = df_train[df_train[\"lang\"] == \"te\"][\"question\"].astype(str)\n",
        "te_val_q   = df_val[df_val[\"lang\"] == \"te\"][\"question\"].astype(str)\n",
        "\n",
        "te_train_punct = Counter(ch for q in te_train_q for ch in PUNCT_RE.findall(q))\n",
        "te_val_punct   = Counter(ch for q in te_val_q for ch in PUNCT_RE.findall(q))\n",
        "\n",
        "print(\"Telugu — TRAIN punctuation (char -> count):\")\n",
        "print(te_train_punct.most_common())\n",
        "print(\"Telugu — VAL punctuation (char -> count):\")\n",
        "print(te_val_punct.most_common())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2b5d529a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b5d529a",
        "outputId": "9f715cd9-d627-471a-ee4b-e87a0ecb3cd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Arabic — TRAIN total words: 16199\n",
            "Arabic — VAL total words: 2617\n",
            "Korean — TRAIN total words: 11858\n",
            "Korean — VAL total words: 1736\n",
            "Telugu — TRAIN total words: 7690\n",
            "Telugu — VAL total words: 2302\n"
          ]
        }
      ],
      "source": [
        "\n",
        "## Each language total words (not counting punctuation)\n",
        "# tokenizer: split on \\W+ (non-word chars); protect hyphens between letters/digits\n",
        "# safeguard: build punctuation set from training+validation data, do not count these tokens as well\n",
        "\n",
        "SPLIT_RE = re.compile(r\"\\W+\", re.UNICODE)          # tokenizer\n",
        "HY = \"HYPHENJOIN\"                                  # placeholder for protected hyphens\n",
        "PROTECT_HYPHEN = re.compile(r\"(?<=[\\p{L}\\p{N}])-(?=[\\p{L}\\p{N}])\", re.UNICODE)  # hyphen between letters/digits\n",
        "\n",
        "# ARABIC\n",
        "ar_train_q = df_train[df_train[\"lang\"] == \"ar\"][\"question\"].astype(str)\n",
        "ar_val_q   = df_val[df_val[\"lang\"] == \"ar\"][\"question\"].astype(str)\n",
        "\n",
        "# build punctuation set (optional safeguard)\n",
        "ar_punct_set = set(ch for q in pd.concat([ar_train_q, ar_val_q]) for ch in PUNCT_RE.findall(q))\n",
        "\n",
        "# protect hyphens, split on \\W+, restore hyphens; slashes will split\n",
        "ar_train_tokens = []\n",
        "for q in ar_train_q:\n",
        "    q2 = PROTECT_HYPHEN.sub(HY, q)\n",
        "    toks = [t.replace(HY, \"-\") for t in SPLIT_RE.split(q2) if t and t not in ar_punct_set]\n",
        "    ar_train_tokens.extend(toks)\n",
        "\n",
        "ar_val_tokens = []\n",
        "for q in ar_val_q:\n",
        "    q2 = PROTECT_HYPHEN.sub(HY, q)\n",
        "    toks = [t.replace(HY, \"-\") for t in SPLIT_RE.split(q2) if t and t not in ar_punct_set]\n",
        "    ar_val_tokens.extend(toks)\n",
        "\n",
        "print(\"Arabic — TRAIN total words:\", len(ar_train_tokens))\n",
        "print(\"Arabic — VAL total words:\",   len(ar_val_tokens))\n",
        "\n",
        "# KOREAN\n",
        "ko_train_q = df_train[df_train[\"lang\"] == \"ko\"][\"question\"].astype(str)\n",
        "ko_val_q   = df_val[df_val[\"lang\"] == \"ko\"][\"question\"].astype(str)\n",
        "\n",
        "ko_punct_set = set(ch for q in pd.concat([ko_train_q, ko_val_q]) for ch in PUNCT_RE.findall(q))\n",
        "\n",
        "ko_train_tokens = []\n",
        "for q in ko_train_q:\n",
        "    q2 = PROTECT_HYPHEN.sub(HY, q)\n",
        "    toks = [t.replace(HY, \"-\") for t in SPLIT_RE.split(q2) if t and t not in ko_punct_set]\n",
        "    ko_train_tokens.extend(toks)\n",
        "\n",
        "ko_val_tokens = []\n",
        "for q in ko_val_q:\n",
        "    q2 = PROTECT_HYPHEN.sub(HY, q)\n",
        "    toks = [t.replace(HY, \"-\") for t in SPLIT_RE.split(q2) if t and t not in ko_punct_set]\n",
        "    ko_val_tokens.extend(toks)\n",
        "\n",
        "print(\"Korean — TRAIN total words:\", len(ko_train_tokens))\n",
        "print(\"Korean — VAL total words:\",   len(ko_val_tokens))\n",
        "\n",
        "# TELUGU\n",
        "te_train_q = df_train[df_train[\"lang\"] == \"te\"][\"question\"].astype(str)\n",
        "te_val_q   = df_val[df_val[\"lang\"] == \"te\"][\"question\"].astype(str)\n",
        "\n",
        "te_punct_set = set(ch for q in pd.concat([te_train_q, te_val_q]) for ch in PUNCT_RE.findall(q))\n",
        "\n",
        "te_train_tokens = []\n",
        "for q in te_train_q:\n",
        "    q2 = PROTECT_HYPHEN.sub(HY, q)\n",
        "    toks = [t.replace(HY, \"-\") for t in SPLIT_RE.split(q2) if t and t not in te_punct_set]\n",
        "    te_train_tokens.extend(toks)\n",
        "\n",
        "te_val_tokens = []\n",
        "for q in te_val_q:\n",
        "    q2 = PROTECT_HYPHEN.sub(HY, q)\n",
        "    toks = [t.replace(HY, \"-\") for t in SPLIT_RE.split(q2) if t and t not in te_punct_set]\n",
        "    te_val_tokens.extend(toks)\n",
        "\n",
        "print(\"Telugu — TRAIN total words:\", len(te_train_tokens))\n",
        "print(\"Telugu — VAL total words:\",   len(te_val_tokens))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4d5ef06c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d5ef06c",
        "outputId": "a90d47eb-0f5c-4b3a-fefe-5f634938ea80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Arabic — numeric tokens (train): 78\n",
            "Arabic — numeric tokens (val): 11\n",
            "Arabic — hyphenated tokens (train): 3\n",
            "Arabic — hyphenated tokens (val): 0\n",
            "Korean — numeric tokens (train): 9\n",
            "Korean — numeric tokens (val): 1\n",
            "Korean — hyphenated tokens (train): 5\n",
            "Korean — hyphenated tokens (val): 1\n",
            "Telugu — numeric tokens (train): 107\n",
            "Telugu — numeric tokens (val): 39\n",
            "Telugu — hyphenated tokens (train): 0\n",
            "Telugu — hyphenated tokens (val): 0\n"
          ]
        }
      ],
      "source": [
        "#Stats on numeric and hyphenated tokens\n",
        "\n",
        "# ---- After tokenization for Arabic ----\n",
        "ar_numbers_train = sum(1 for t in ar_train_tokens if t.isdigit())\n",
        "ar_numbers_val   = sum(1 for t in ar_val_tokens if t.isdigit())\n",
        "\n",
        "ar_hyphen_train = sum(1 for t in ar_train_tokens if \"-\" in t)\n",
        "ar_hyphen_val   = sum(1 for t in ar_val_tokens if \"-\" in t)\n",
        "\n",
        "print(\"Arabic — numeric tokens (train):\", ar_numbers_train)\n",
        "print(\"Arabic — numeric tokens (val):\",   ar_numbers_val)\n",
        "print(\"Arabic — hyphenated tokens (train):\", ar_hyphen_train)\n",
        "print(\"Arabic — hyphenated tokens (val):\",   ar_hyphen_val)\n",
        "\n",
        "# ---- After tokenization for Korean ----\n",
        "ko_numbers_train = sum(1 for t in ko_train_tokens if t.isdigit())\n",
        "ko_numbers_val   = sum(1 for t in ko_val_tokens if t.isdigit())\n",
        "\n",
        "ko_hyphen_train = sum(1 for t in ko_train_tokens if \"-\" in t)\n",
        "ko_hyphen_val   = sum(1 for t in ko_val_tokens if \"-\" in t)\n",
        "\n",
        "print(\"Korean — numeric tokens (train):\", ko_numbers_train)\n",
        "print(\"Korean — numeric tokens (val):\",   ko_numbers_val)\n",
        "print(\"Korean — hyphenated tokens (train):\", ko_hyphen_train)\n",
        "print(\"Korean — hyphenated tokens (val):\",   ko_hyphen_val)\n",
        "\n",
        "# ---- After tokenization for Telugu ----\n",
        "te_numbers_train = sum(1 for t in te_train_tokens if t.isdigit())\n",
        "te_numbers_val   = sum(1 for t in te_val_tokens if t.isdigit())\n",
        "\n",
        "te_hyphen_train = sum(1 for t in te_train_tokens if \"-\" in t)\n",
        "te_hyphen_val   = sum(1 for t in te_val_tokens if \"-\" in t)\n",
        "\n",
        "print(\"Telugu — numeric tokens (train):\", te_numbers_train)\n",
        "print(\"Telugu — numeric tokens (val):\",   te_numbers_val)\n",
        "print(\"Telugu — hyphenated tokens (train):\", te_hyphen_train)\n",
        "print(\"Telugu — hyphenated tokens (val):\",   te_hyphen_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0e346b64",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e346b64",
        "outputId": "f2b9ee7b-32d8-428a-e6a2-aaaada275007"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Arabic — Top 5 most common words (TRAIN):\n",
            "في\tcount=593\t→ in\n",
            "من\tcount=587\t→ from\n",
            "متى\tcount=536\t→ when\n",
            "ما\tcount=443\t→ what\n",
            "هو\tcount=350\t→ he\n",
            "\n",
            "Korean — Top 5 most common words (TRAIN):\n",
            "가장\tcount=527\t→ most\n",
            "무엇인가\tcount=497\t→ Something\n",
            "언제\tcount=336\t→ when\n",
            "몇\tcount=234\t→ some\n",
            "어디인가\tcount=228\t→ Where\n",
            "\n",
            "Telugu — Top 5 most common words (TRAIN):\n",
            "ఎవరు\tcount=274\t→ Who is\n",
            "ఏది\tcount=192\t→ Which one is\n",
            "ఎన్ని\tcount=165\t→ How many\n",
            "ఎప్పుడు\tcount=154\t→ When\n",
            "ఏ\tcount=144\t→ A.\n"
          ]
        }
      ],
      "source": [
        "#5 Most common words (not counting punctuation); with English translations and their count\n",
        "\n",
        "translator = Translator()\n",
        "\n",
        "# ARABIC\n",
        "# (skip pure numbers)\n",
        "ar_counts = Counter([t.lower() for t in ar_train_tokens if t ])\n",
        "ar_top5 = ar_counts.most_common(5)\n",
        "\n",
        "print(\"Arabic — Top 5 most common words (TRAIN):\")\n",
        "for w, c in ar_top5:\n",
        "    try:\n",
        "        en = translator.translate(w, src='ar', dest='en').text\n",
        "    except Exception as e:\n",
        "        en = f\"[translation error: {e}]\"\n",
        "    print(f\"{w}\\tcount={c}\\t→ {en}\")\n",
        "\n",
        "# KOREAN\n",
        "ko_counts = Counter([t.lower() for t in ko_train_tokens if t ])\n",
        "ko_top5 = ko_counts.most_common(5)\n",
        "\n",
        "print(\"\\nKorean — Top 5 most common words (TRAIN):\")\n",
        "for w, c in ko_top5:\n",
        "    try:\n",
        "        en = translator.translate(w, src='ko', dest='en').text\n",
        "    except Exception as e:\n",
        "        en = f\"[translation error: {e}]\"\n",
        "    print(f\"{w}\\tcount={c}\\t→ {en}\")\n",
        "\n",
        "#  TELUGU\n",
        "te_counts = Counter([t.lower() for t in te_train_tokens if t])\n",
        "te_top5 = te_counts.most_common(5)\n",
        "\n",
        "print(\"\\nTelugu — Top 5 most common words (TRAIN):\")\n",
        "for w, c in te_top5:\n",
        "    try:\n",
        "        en = translator.translate(w, src='te', dest='en').text\n",
        "    except Exception as e:\n",
        "        en = f\"[translation error: {e}]\"\n",
        "    print(f\"{w}\\tcount={c}\\t→ {en}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "287a0a3d",
      "metadata": {
        "id": "287a0a3d"
      },
      "source": [
        "### We conclude the words are \"stop words\" that we learned in the lecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d4a95d97",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4a95d97",
        "outputId": "9fa97513-ab33-4eff-ce9c-b0740dd8d52b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split Language  Total  Answerable  Unanswerable  Answerable Ratio\n",
            "train       ar   2558        2303           255          0.900313\n",
            "train       ko   2422        2359            63          0.973988\n",
            "train       te   1355        1310            45          0.966790\n",
            "  val       ar    415         363            52          0.874699\n",
            "  val       ko    356         337            19          0.946629\n",
            "  val       te    384         291            93          0.757812\n"
          ]
        }
      ],
      "source": [
        "# Stats about answerable vs unanswerable questions\n",
        "\n",
        "# Define languages and splits\n",
        "\n",
        "split_dfs = {\n",
        "    \"train\": df_train,\n",
        "    \"val\":   df_val\n",
        "}\n",
        "\n",
        "\n",
        "rows = []\n",
        "for split_name, df in split_dfs.items():\n",
        "    for lang in langs:\n",
        "        total = df[df[\"lang\"] == lang].shape[0]\n",
        "        ans   = df[(df[\"lang\"] == lang) & (df[\"answerable\"])].shape[0]\n",
        "        unans = total - ans\n",
        "        ratio = ans / total if total > 0 else 0\n",
        "        rows.append([split_name, lang, total, ans, unans, ratio])\n",
        "\n",
        "# Create summary DataFrame\n",
        "summary = pd.DataFrame(rows, columns=[\"Split\", \"Language\", \"Total\", \"Answerable\", \"Unanswerable\", \"Answerable Ratio\"])\n",
        "print(summary.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd5a706b",
      "metadata": {
        "id": "bd5a706b"
      },
      "source": [
        "## RULE BASE CLASSIFIER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b60KL8ViRNKn",
      "metadata": {
        "id": "b60KL8ViRNKn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import regex as re\n",
        "import string\n",
        "from unidecode import unidecode\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "# Needed for word_tokenize\n",
        "try:\n",
        "    nltk.download(\"punkt\", quiet=True)\n",
        "    # Some NLTK builds need this extra package\n",
        "    nltk.download(\"punkt_tab\", quiet=True)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "EN_STOP = set(stopwords.words('english')) | set(string.punctuation)\n",
        "\n",
        "# If you haven't already created these:\n",
        "# df_train = pd.read_csv(\"path/to/train.csv\")  # or load however you have it\n",
        "# df_val   = pd.read_csv(\"path/to/val.csv\")\n",
        "\n",
        "# Languages you care about\n",
        "LANGS = [\"ar\", \"ko\", \"te\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "57dkoe5pRQId",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57dkoe5pRQId",
        "outputId": "9cc3a6c3-7497-4c5e-afa0-ea7047e798e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "Using device idx for HF pipeline: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "print(\"PyTorch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "DEVICE = 0 if torch.cuda.is_available() else -1\n",
        "print(\"Using device idx for HF pipeline:\", DEVICE)\n",
        "\n",
        "MODEL_ID = \"facebook/nllb-200-distilled-600M\"\n",
        "SRC_CODES = {\"ar\": \"arb_Arab\", \"ko\": \"kor_Hang\", \"te\": \"tel_Telu\"}\n",
        "TGT_CODE = \"eng_Latn\"\n",
        "\n",
        "# One translator pipeline reused for all batches\n",
        "nllb = pipeline(\"translation\", model=MODEL_ID, tokenizer=MODEL_ID, device=DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "470eea80",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Iterable, Dict, Optional\n",
        "from tqdm import tqdm\n",
        "\n",
        "def translate_list_pipe(texts: List[str],\n",
        "                        src_lang: str,\n",
        "                        batch_size: int = 128,\n",
        "                        max_length: int = 320,\n",
        "                        show_progress: bool = True) -> List[str]:\n",
        "    \"\"\"\n",
        "    Translate a list of strings to English using NLLB. Robust to errors; returns \"\" on failure.\n",
        "    Assumes src_lang is one of LANGS and uses SRC_CODES mapping.\n",
        "    \"\"\"\n",
        "    if src_lang not in SRC_CODES:\n",
        "        raise ValueError(f\"Unknown lang code '{src_lang}'. Expected one of {list(SRC_CODES)}\")\n",
        "\n",
        "    outputs = []\n",
        "    iterator = range(0, len(texts), batch_size)\n",
        "    if show_progress:\n",
        "        iterator = tqdm(iterator, total=(len(texts) + batch_size - 1)//batch_size, desc=f\"Translating {src_lang}->EN\")\n",
        "\n",
        "    for i in iterator:\n",
        "        batch = [x if isinstance(x, str) else \"\" for x in texts[i:i+batch_size]]\n",
        "        try:\n",
        "            preds = nllb(\n",
        "                batch,\n",
        "                src_lang=SRC_CODES[src_lang],\n",
        "                tgt_lang=TGT_CODE,\n",
        "                truncation=True,\n",
        "                max_length=max_length\n",
        "            )\n",
        "            outputs.extend([p.get(\"translation_text\", \"\") for p in preds])\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Batch {i}:{i+len(batch)} failed: {type(e).__name__}: {e}\")\n",
        "            outputs.extend([\"\"] * len(batch))\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9577d0d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def ensure_column(df: pd.DataFrame, col: str):\n",
        "    if col not in df.columns:\n",
        "        df[col] = pd.Series([np.nan]*len(df), index=df.index)\n",
        "\n",
        "# Ensure the target columns exist\n",
        "ensure_column(df_train, \"question_en\")\n",
        "ensure_column(df_val,   \"question_en\")\n",
        "\n",
        "# Toggle if you want to translate contexts too (usually NOT needed for TyDi XOR RC)\n",
        "TRANSLATE_CONTEXT = False\n",
        "if TRANSLATE_CONTEXT:\n",
        "    ensure_column(df_train, \"context_en\")\n",
        "    ensure_column(df_val,   \"context_en\")\n",
        "\n",
        "def cache_translations(df: pd.DataFrame,\n",
        "                       text_col: str,\n",
        "                       out_col: str,\n",
        "                       langs: Iterable[str] = LANGS,\n",
        "                       batch_size: int = 128):\n",
        "    \"\"\"\n",
        "    For each lang in langs, fills df[out_col] with EN translations of df[text_col] where it's missing/NaN.\n",
        "    \"\"\"\n",
        "    for lg in langs:\n",
        "        mask_lang = (df[\"lang\"] == lg)\n",
        "        mask_need = df[out_col].isna() | (df[out_col].astype(str).str.strip() == \"\")\n",
        "        mask = mask_lang & mask_need\n",
        "        if not mask.any():\n",
        "            print(f\"[{text_col}] '{lg}' — already cached, skipping.\")\n",
        "            continue\n",
        "\n",
        "        texts = df.loc[mask, text_col].astype(str).tolist()\n",
        "        print(f\"[{text_col}] Translating {sum(mask)} rows for lang='{lg}'...\")\n",
        "        df.loc[mask, out_col] = translate_list_pipe(texts, src_lang=lg, batch_size=batch_size)\n",
        "\n",
        "# Cache questions\n",
        "cache_translations(df_train, text_col=\"question\", out_col=\"question_en\", batch_size=128)\n",
        "cache_translations(df_val,   text_col=\"question\", out_col=\"question_en\", batch_size=128)\n",
        "\n",
        "# Optionally cache contexts (only if you set TRANSLATE_CONTEXT=True)\n",
        "if TRANSLATE_CONTEXT:\n",
        "    cache_translations(df_train, text_col=\"context\", out_col=\"context_en\", batch_size=128)\n",
        "    cache_translations(df_val,   text_col=\"context\", out_col=\"context_en\", batch_size=128)\n",
        "\n",
        "# (Optional) Persist to disk so you never re-translate again\n",
        "df_train.to_parquet(\"df_train_translated.parquet\")\n",
        "df_val.to_parquet(\"df_val_translated.parquet\")\n",
        "print(\"Saved: df_train_translated.parquet, df_val_translated.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff8cda35",
      "metadata": {},
      "outputs": [],
      "source": [
        "def sanity_check_translations(df, text_col=\"question\", trans_col=\"question_en\", langs=LANGS, n=3):\n",
        "    \"\"\"\n",
        "    Prints n sample original vs. translated texts for each language to verify translation.\n",
        "    \"\"\"\n",
        "    for lg in langs:\n",
        "        subset = df[df[\"lang\"] == lg].dropna(subset=[text_col, trans_col])\n",
        "        if subset.empty:\n",
        "            print(f\"[WARN] No data for lang '{lg}' in {text_col}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n=== {lg.upper()} → EN (showing {n} samples) ===\")\n",
        "        for i, row in subset.head(n).iterrows():\n",
        "            print(\"SRC:\", row[text_col])\n",
        "            print(\"EN :\", row[trans_col])\n",
        "            print(\"-\"*40)\n",
        "\n",
        "# Run sanity check on train set\n",
        "sanity_check_translations(df_train, text_col=\"question\", trans_col=\"question_en\", langs=LANGS, n=3)\n",
        "\n",
        "# Optionally check val set too\n",
        "sanity_check_translations(df_val, text_col=\"question\", trans_col=\"question_en\", langs=LANGS, n=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6u-uk8uni3mA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6u-uk8uni3mA",
        "outputId": "981853d2-fb9b-4876-dc6f-7b2b0db87e45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        n_train  n_val  train_acc  val_acc  val_prec  val_rec  val_f1  min_matches  min_ratio\n",
            "Arabic   2558.0  415.0     0.8170   0.7783    0.8839   0.8595  0.8715          1.0        0.3\n",
            "Korean   2422.0  356.0     0.8365   0.7949    0.9521   0.8249  0.8839          1.0        0.3\n",
            "Telugu   1355.0  384.0     0.8738   0.7760    0.8042   0.9313  0.8631          1.0        0.3\n",
            "\n",
            "Confusion matrices:\n",
            "Arabic: {'TP': 312, 'FP': 41, 'FN': 51, 'TN': 11}\n",
            "Korean: {'TP': 278, 'FP': 14, 'FN': 59, 'TN': 5}\n",
            "Telugu: {'TP': 271, 'FP': 66, 'FN': 20, 'TN': 27}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import regex as re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "nltk.download('stopwords', quiet=True)\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "STOP_WORDS = set(stopwords.words('english')) | set(string.punctuation)\n",
        "\n",
        "\n",
        "def pick_cols(df, translate_contexts=False):\n",
        "    q_col = 'question_en' if 'question_en' in df.columns else 'question'\n",
        "    if translate_contexts and 'context_en' in df.columns:\n",
        "        c_col = 'context_en'\n",
        "    else:\n",
        "        c_col = 'context'\n",
        "    return q_col, c_col\n",
        "\n",
        "\n",
        "def tokenize(text: str):\n",
        "    tokens = re.split(r'\\W+', str(text) if text is not None else \"\")\n",
        "    return [t.lower() for t in tokens if t and t.lower() not in STOP_WORDS]\n",
        "\n",
        "def overlap_score_question(question: str, context: str):\n",
        "    q_toks = tokenize(question)\n",
        "    c_toks = tokenize(context)\n",
        "    if not q_toks:\n",
        "        return 0.0, 0\n",
        "    matched = set()\n",
        "    for q in q_toks:\n",
        "        for c in c_toks:\n",
        "            if q == c or (q in c) or (c in q):\n",
        "                matched.add(q)\n",
        "                break\n",
        "    matches = len(matched)\n",
        "    ratio = matches / max(1, len(q_toks))\n",
        "    return ratio, matches\n",
        "\n",
        "def tune_parameters(train_df, q_col, c_col,\n",
        "                    match_grid=(1,2,3,4,5,6,7,8,9,10),\n",
        "                    thr_grid=(0.3,0.4,0.5,0.6,0.7,0.8,0.9)):\n",
        "    data = [(overlap_score_question(getattr(r, q_col), getattr(r, c_col)), int(r.answerable))\n",
        "            for r in train_df.itertuples(index=False)]\n",
        "    best_acc, best_k, best_thr = 0.0, 1, 0.5\n",
        "    for k in match_grid:\n",
        "        for thr in thr_grid:\n",
        "            correct = 0\n",
        "            for (ratio, m), y in data:\n",
        "                pred = int((m >= k) and (ratio >= thr))\n",
        "                correct += (pred == y)\n",
        "            acc = correct / len(data) if data else 0.0\n",
        "            if acc > best_acc:\n",
        "                best_acc, best_k, best_thr = acc, k, thr\n",
        "    return {\"min_match_count\": best_k, \"min_ratio_threshold\": best_thr, \"best_train_acc\": best_acc}\n",
        "\n",
        "\n",
        "def eval_metrics(df, q_col, c_col, min_matches, ratio_threshold):\n",
        "    y_true, y_pred = [], []\n",
        "    for r in df.itertuples(index=False):\n",
        "        ratio, m = overlap_score_question(getattr(r, q_col), getattr(r, c_col))\n",
        "        y_true.append(int(r.answerable))\n",
        "        y_pred.append(int((m >= min_matches) and (ratio >= ratio_threshold)))\n",
        "    y_true = np.asarray(y_true, int)\n",
        "    y_pred = np.asarray(y_pred, int)\n",
        "\n",
        "    tp = int(((y_pred==1) & (y_true==1)).sum())\n",
        "    fp = int(((y_pred==1) & (y_true==0)).sum())\n",
        "    fn = int(((y_pred==0) & (y_true==1)).sum())\n",
        "    tn = int(((y_pred==0) & (y_true==0)).sum())\n",
        "\n",
        "    acc  = (tp+tn)/max(1, tp+tn+fp+fn)\n",
        "    prec = tp/max(1, tp+fp)\n",
        "    rec  = tp/max(1, tp+fn)\n",
        "    f1   = 0.0 if (prec+rec)==0 else 2*prec*rec/(prec+rec)\n",
        "\n",
        "    return {\n",
        "        \"acc\": round(acc, 4),\n",
        "        \"prec\": round(prec, 4),\n",
        "        \"rec\": round(rec, 4),\n",
        "        \"f1\": round(f1, 4),\n",
        "        \"cm\": {\"TP\": tp, \"FP\": fp, \"FN\": fn, \"TN\": tn}\n",
        "    }\n",
        "\n",
        "\n",
        "def run_rule_classifier(df_train, df_val, translate_contexts=False):\n",
        "    results = {}\n",
        "    for code, name in [(\"ar\",\"Arabic\"), (\"ko\",\"Korean\"), (\"te\",\"Telugu\")]:\n",
        "        tr = df_train[df_train[\"lang\"] == code].copy()\n",
        "        va = df_val[df_val[\"lang\"] == code].copy()\n",
        "        if tr.empty or va.empty:\n",
        "            results[name] = {\n",
        "                \"train_acc\": None, \"val_acc\": None, \"val_prec\": None, \"val_rec\": None, \"val_f1\": None,\n",
        "                \"min_matches\": None, \"min_ratio\": None, \"cm\": None, \"n_train\": len(tr), \"n_val\": len(va)\n",
        "            }\n",
        "            continue\n",
        "\n",
        "        q_col_tr, c_col_tr = pick_cols(tr, translate_contexts=translate_contexts)\n",
        "        q_col_va, c_col_va = pick_cols(va, translate_contexts=translate_contexts)\n",
        "\n",
        "        params = tune_parameters(tr, q_col_tr, c_col_tr)\n",
        "        metrics_val = eval_metrics(va, q_col_va, c_col_va,\n",
        "                                   params[\"min_match_count\"], params[\"min_ratio_threshold\"])\n",
        "\n",
        "        results[name] = {\n",
        "            \"n_train\": len(tr),\n",
        "            \"n_val\": len(va),\n",
        "            \"train_acc\": round(params[\"best_train_acc\"], 4),\n",
        "            \"val_acc\": metrics_val[\"acc\"],\n",
        "            \"val_prec\": metrics_val[\"prec\"],\n",
        "            \"val_rec\": metrics_val[\"rec\"],\n",
        "            \"val_f1\": metrics_val[\"f1\"],\n",
        "            \"min_matches\": params[\"min_match_count\"],\n",
        "            \"min_ratio\": params[\"min_ratio_threshold\"],\n",
        "            \"cm\": metrics_val[\"cm\"],\n",
        "        }\n",
        "\n",
        " \n",
        "    summary = pd.DataFrame({\n",
        "        lang: {k:v for k,v in res.items() if k not in (\"cm\",)}\n",
        "        for lang, res in results.items()\n",
        "    }).T\n",
        "    print(summary.to_string())\n",
        "\n",
        "    # Also print confusion matrices\n",
        "    print(\"\\nConfusion matrices:\")\n",
        "    for lang, res in results.items():\n",
        "        print(f\"{lang}: {res['cm']}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "_ = run_rule_classifier(df_train, df_val, translate_contexts=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfSBMfPHjoJG",
      "metadata": {
        "id": "cfSBMfPHjoJG"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
