{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfc3380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##IMPORTS\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from collections import Counter\n",
    "from transformers import pipeline\n",
    "from googletrans import Translator\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import regex as re\n",
    "from collections import Counter\n",
    "from googletrans import Translator\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13178b08",
   "metadata": {},
   "source": [
    "# WEEK 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a67142ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOWNLOAD DATASET\n",
    "\n",
    "splits = {'train': 'train.parquet', 'validation': 'validation.parquet'}\n",
    "df_train = pd.read_parquet(\"hf://datasets/coastalcph/tydi_xor_rc/\" + splits[\"train\"])\n",
    "df_val = pd.read_parquet(\"hf://datasets/coastalcph/tydi_xor_rc/\" + splits[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0aac73",
   "metadata": {},
   "source": [
    "## Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "316f9bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes for selected languages:\n",
      "      train_size  val_size\n",
      "lang                      \n",
      "ar          2558       415\n",
      "ko          2422       356\n",
      "te          1355       384\n"
     ]
    }
   ],
   "source": [
    "#STATS\n",
    "\n",
    "#SIZE\n",
    "\n",
    "langs = [\"ar\", \"ko\", \"te\"]\n",
    "\n",
    "\n",
    "train_counts = df_train[df_train[\"lang\"].isin(langs)].groupby(\"lang\").size()\n",
    "\n",
    "\n",
    "val_counts = df_val[df_val[\"lang\"].isin(langs)].groupby(\"lang\").size()\n",
    "\n",
    "size_df = pd.DataFrame({\n",
    "    \"train_size\": train_counts,\n",
    "    \"val_size\": val_counts\n",
    "}).fillna(0).astype(int)\n",
    "\n",
    "print(\"Dataset sizes for selected languages:\")\n",
    "print(size_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "905939a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabic — TRAIN punctuation (char -> count):\n",
      "[('؟', 2556), ('\"', 80), ('(', 25), (')', 25), ('-', 5), ('.', 2), ('/', 2), ('«', 2), ('»', 2), ('_', 2), ('\\\\', 1), ('—', 1), ('!', 1), ('،', 1)]\n",
      "Arabic — VAL punctuation (char -> count):\n",
      "[('؟', 413), ('\"', 4), ('(', 3), (')', 3), ('،', 1), ('-', 1)]\n",
      "Korean — TRAIN punctuation (char -> count):\n",
      "[('?', 2420), (',', 23), ('.', 16), (\"'\", 6), ('\"', 6), ('-', 5), (':', 2), ('/', 1), ('\\\\', 1), ('(', 1), (')', 1)]\n",
      "Korean — VAL punctuation (char -> count):\n",
      "[('?', 356), ('.', 9), (',', 3), ('-', 1)]\n",
      "Telugu — TRAIN punctuation (char -> count):\n",
      "[('?', 1355), ('.', 42), (',', 6), ('-', 3), ('%', 1), ('–', 1)]\n",
      "Telugu — VAL punctuation (char -> count):\n",
      "[('?', 384), ('.', 2), ('-', 1), ('%', 1)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Each language punctuation \n",
    "PUNCT_RE = re.compile(r\"\\p{P}\", re.UNICODE)\n",
    "\n",
    "# ARABIC\n",
    "ar_train_q = df_train[df_train[\"lang\"] == \"ar\"][\"question\"].astype(str)\n",
    "ar_val_q   = df_val[df_val[\"lang\"] == \"ar\"][\"question\"].astype(str)\n",
    "\n",
    "ar_train_punct = Counter(ch for q in ar_train_q for ch in PUNCT_RE.findall(q))\n",
    "ar_val_punct   = Counter(ch for q in ar_val_q for ch in PUNCT_RE.findall(q))\n",
    "\n",
    "print(\"Arabic — TRAIN punctuation (char -> count):\")\n",
    "print(ar_train_punct.most_common())\n",
    "print(\"Arabic — VAL punctuation (char -> count):\")\n",
    "print(ar_val_punct.most_common())\n",
    "\n",
    "# KOREAN\n",
    "ko_train_q = df_train[df_train[\"lang\"] == \"ko\"][\"question\"].astype(str)\n",
    "ko_val_q   = df_val[df_val[\"lang\"] == \"ko\"][\"question\"].astype(str)\n",
    "\n",
    "ko_train_punct = Counter(ch for q in ko_train_q for ch in PUNCT_RE.findall(q))\n",
    "ko_val_punct   = Counter(ch for q in ko_val_q for ch in PUNCT_RE.findall(q))\n",
    "\n",
    "print(\"Korean — TRAIN punctuation (char -> count):\")\n",
    "print(ko_train_punct.most_common())\n",
    "print(\"Korean — VAL punctuation (char -> count):\")\n",
    "print(ko_val_punct.most_common())\n",
    "\n",
    "# TELUGU\n",
    "te_train_q = df_train[df_train[\"lang\"] == \"te\"][\"question\"].astype(str)\n",
    "te_val_q   = df_val[df_val[\"lang\"] == \"te\"][\"question\"].astype(str)\n",
    "\n",
    "te_train_punct = Counter(ch for q in te_train_q for ch in PUNCT_RE.findall(q))\n",
    "te_val_punct   = Counter(ch for q in te_val_q for ch in PUNCT_RE.findall(q))\n",
    "\n",
    "print(\"Telugu — TRAIN punctuation (char -> count):\")\n",
    "print(te_train_punct.most_common())\n",
    "print(\"Telugu — VAL punctuation (char -> count):\")\n",
    "print(te_val_punct.most_common())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b5d529a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabic — TRAIN total words: 16199\n",
      "Arabic — VAL total words: 2617\n",
      "Korean — TRAIN total words: 11858\n",
      "Korean — VAL total words: 1736\n",
      "Telugu — TRAIN total words: 7690\n",
      "Telugu — VAL total words: 2302\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Each language total words (not counting punctuation)\n",
    "# tokenizer: split on \\W+ (non-word chars); protect hyphens between letters/digits\n",
    "# safeguard: build punctuation set from training+validation data, do not count these tokens as well\n",
    "\n",
    "SPLIT_RE = re.compile(r\"\\W+\", re.UNICODE)          # tokenizer\n",
    "HY = \"HYPHENJOIN\"                                  # placeholder for protected hyphens\n",
    "PROTECT_HYPHEN = re.compile(r\"(?<=[\\p{L}\\p{N}])-(?=[\\p{L}\\p{N}])\", re.UNICODE)  # hyphen between letters/digits\n",
    "\n",
    "# ARABIC\n",
    "ar_train_q = df_train[df_train[\"lang\"] == \"ar\"][\"question\"].astype(str)\n",
    "ar_val_q   = df_val[df_val[\"lang\"] == \"ar\"][\"question\"].astype(str)\n",
    "\n",
    "# build punctuation set (optional safeguard)\n",
    "ar_punct_set = set(ch for q in pd.concat([ar_train_q, ar_val_q]) for ch in PUNCT_RE.findall(q))\n",
    "\n",
    "# protect hyphens, split on \\W+, restore hyphens; slashes will split\n",
    "ar_train_tokens = []\n",
    "for q in ar_train_q:\n",
    "    q2 = PROTECT_HYPHEN.sub(HY, q)\n",
    "    toks = [t.replace(HY, \"-\") for t in SPLIT_RE.split(q2) if t and t not in ar_punct_set]\n",
    "    ar_train_tokens.extend(toks)\n",
    "\n",
    "ar_val_tokens = []\n",
    "for q in ar_val_q:\n",
    "    q2 = PROTECT_HYPHEN.sub(HY, q)\n",
    "    toks = [t.replace(HY, \"-\") for t in SPLIT_RE.split(q2) if t and t not in ar_punct_set]\n",
    "    ar_val_tokens.extend(toks)\n",
    "\n",
    "print(\"Arabic — TRAIN total words:\", len(ar_train_tokens))\n",
    "print(\"Arabic — VAL total words:\",   len(ar_val_tokens))\n",
    "\n",
    "# KOREAN\n",
    "ko_train_q = df_train[df_train[\"lang\"] == \"ko\"][\"question\"].astype(str)\n",
    "ko_val_q   = df_val[df_val[\"lang\"] == \"ko\"][\"question\"].astype(str)\n",
    "\n",
    "ko_punct_set = set(ch for q in pd.concat([ko_train_q, ko_val_q]) for ch in PUNCT_RE.findall(q))\n",
    "\n",
    "ko_train_tokens = []\n",
    "for q in ko_train_q:\n",
    "    q2 = PROTECT_HYPHEN.sub(HY, q)\n",
    "    toks = [t.replace(HY, \"-\") for t in SPLIT_RE.split(q2) if t and t not in ko_punct_set]\n",
    "    ko_train_tokens.extend(toks)\n",
    "\n",
    "ko_val_tokens = []\n",
    "for q in ko_val_q:\n",
    "    q2 = PROTECT_HYPHEN.sub(HY, q)\n",
    "    toks = [t.replace(HY, \"-\") for t in SPLIT_RE.split(q2) if t and t not in ko_punct_set]\n",
    "    ko_val_tokens.extend(toks)\n",
    "\n",
    "print(\"Korean — TRAIN total words:\", len(ko_train_tokens))\n",
    "print(\"Korean — VAL total words:\",   len(ko_val_tokens))\n",
    "\n",
    "# TELUGU\n",
    "te_train_q = df_train[df_train[\"lang\"] == \"te\"][\"question\"].astype(str)\n",
    "te_val_q   = df_val[df_val[\"lang\"] == \"te\"][\"question\"].astype(str)\n",
    "\n",
    "te_punct_set = set(ch for q in pd.concat([te_train_q, te_val_q]) for ch in PUNCT_RE.findall(q))\n",
    "\n",
    "te_train_tokens = []\n",
    "for q in te_train_q:\n",
    "    q2 = PROTECT_HYPHEN.sub(HY, q)\n",
    "    toks = [t.replace(HY, \"-\") for t in SPLIT_RE.split(q2) if t and t not in te_punct_set]\n",
    "    te_train_tokens.extend(toks)\n",
    "\n",
    "te_val_tokens = []\n",
    "for q in te_val_q:\n",
    "    q2 = PROTECT_HYPHEN.sub(HY, q)\n",
    "    toks = [t.replace(HY, \"-\") for t in SPLIT_RE.split(q2) if t and t not in te_punct_set]\n",
    "    te_val_tokens.extend(toks)\n",
    "\n",
    "print(\"Telugu — TRAIN total words:\", len(te_train_tokens))\n",
    "print(\"Telugu — VAL total words:\",   len(te_val_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d5ef06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabic — numeric tokens (train): 78\n",
      "Arabic — numeric tokens (val): 11\n",
      "Arabic — hyphenated tokens (train): 3\n",
      "Arabic — hyphenated tokens (val): 0\n",
      "Korean — numeric tokens (train): 9\n",
      "Korean — numeric tokens (val): 1\n",
      "Korean — hyphenated tokens (train): 5\n",
      "Korean — hyphenated tokens (val): 1\n",
      "Telugu — numeric tokens (train): 107\n",
      "Telugu — numeric tokens (val): 39\n",
      "Telugu — hyphenated tokens (train): 0\n",
      "Telugu — hyphenated tokens (val): 0\n"
     ]
    }
   ],
   "source": [
    "#Stats on numeric and hyphenated tokens \n",
    "\n",
    "# ---- After tokenization for Arabic ----\n",
    "ar_numbers_train = sum(1 for t in ar_train_tokens if t.isdigit())\n",
    "ar_numbers_val   = sum(1 for t in ar_val_tokens if t.isdigit())\n",
    "\n",
    "ar_hyphen_train = sum(1 for t in ar_train_tokens if \"-\" in t)\n",
    "ar_hyphen_val   = sum(1 for t in ar_val_tokens if \"-\" in t)\n",
    "\n",
    "print(\"Arabic — numeric tokens (train):\", ar_numbers_train)\n",
    "print(\"Arabic — numeric tokens (val):\",   ar_numbers_val)\n",
    "print(\"Arabic — hyphenated tokens (train):\", ar_hyphen_train)\n",
    "print(\"Arabic — hyphenated tokens (val):\",   ar_hyphen_val)\n",
    "\n",
    "# ---- After tokenization for Korean ----\n",
    "ko_numbers_train = sum(1 for t in ko_train_tokens if t.isdigit())\n",
    "ko_numbers_val   = sum(1 for t in ko_val_tokens if t.isdigit())\n",
    "\n",
    "ko_hyphen_train = sum(1 for t in ko_train_tokens if \"-\" in t)\n",
    "ko_hyphen_val   = sum(1 for t in ko_val_tokens if \"-\" in t)\n",
    "\n",
    "print(\"Korean — numeric tokens (train):\", ko_numbers_train)\n",
    "print(\"Korean — numeric tokens (val):\",   ko_numbers_val)\n",
    "print(\"Korean — hyphenated tokens (train):\", ko_hyphen_train)\n",
    "print(\"Korean — hyphenated tokens (val):\",   ko_hyphen_val)\n",
    "\n",
    "# ---- After tokenization for Telugu ----\n",
    "te_numbers_train = sum(1 for t in te_train_tokens if t.isdigit())\n",
    "te_numbers_val   = sum(1 for t in te_val_tokens if t.isdigit())\n",
    "\n",
    "te_hyphen_train = sum(1 for t in te_train_tokens if \"-\" in t)\n",
    "te_hyphen_val   = sum(1 for t in te_val_tokens if \"-\" in t)\n",
    "\n",
    "print(\"Telugu — numeric tokens (train):\", te_numbers_train)\n",
    "print(\"Telugu — numeric tokens (val):\",   te_numbers_val)\n",
    "print(\"Telugu — hyphenated tokens (train):\", te_hyphen_train)\n",
    "print(\"Telugu — hyphenated tokens (val):\",   te_hyphen_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e346b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabic — Top 5 most common words (TRAIN):\n",
      "في\tcount=593\t→ in\n",
      "من\tcount=587\t→ from\n",
      "متى\tcount=536\t→ when\n",
      "ما\tcount=443\t→ what\n",
      "هو\tcount=350\t→ he\n",
      "\n",
      "Korean — Top 5 most common words (TRAIN):\n",
      "가장\tcount=527\t→ most\n",
      "무엇인가\tcount=497\t→ Something\n",
      "언제\tcount=336\t→ when\n",
      "몇\tcount=234\t→ some\n",
      "어디인가\tcount=228\t→ Where\n",
      "\n",
      "Telugu — Top 5 most common words (TRAIN):\n",
      "ఎవరు\tcount=274\t→ Who is\n",
      "ఏది\tcount=192\t→ Which one is\n",
      "ఎన్ని\tcount=165\t→ How many\n",
      "ఎప్పుడు\tcount=154\t→ When\n",
      "ఏ\tcount=144\t→ A.\n"
     ]
    }
   ],
   "source": [
    "#5 Most common words (not counting punctuation); with English translations and their count\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "# ARABIC \n",
    "# (skip pure numbers)\n",
    "ar_counts = Counter([t.lower() for t in ar_train_tokens if t ])\n",
    "ar_top5 = ar_counts.most_common(5)\n",
    "\n",
    "print(\"Arabic — Top 5 most common words (TRAIN):\")\n",
    "for w, c in ar_top5:\n",
    "    try:\n",
    "        en = translator.translate(w, src='ar', dest='en').text\n",
    "    except Exception as e:\n",
    "        en = f\"[translation error: {e}]\"\n",
    "    print(f\"{w}\\tcount={c}\\t→ {en}\")\n",
    "\n",
    "# KOREAN \n",
    "ko_counts = Counter([t.lower() for t in ko_train_tokens if t ])\n",
    "ko_top5 = ko_counts.most_common(5)\n",
    "\n",
    "print(\"\\nKorean — Top 5 most common words (TRAIN):\")\n",
    "for w, c in ko_top5:\n",
    "    try:\n",
    "        en = translator.translate(w, src='ko', dest='en').text\n",
    "    except Exception as e:\n",
    "        en = f\"[translation error: {e}]\"\n",
    "    print(f\"{w}\\tcount={c}\\t→ {en}\")\n",
    "\n",
    "#  TELUGU\n",
    "te_counts = Counter([t.lower() for t in te_train_tokens if t])\n",
    "te_top5 = te_counts.most_common(5)\n",
    "\n",
    "print(\"\\nTelugu — Top 5 most common words (TRAIN):\")\n",
    "for w, c in te_top5:\n",
    "    try:\n",
    "        en = translator.translate(w, src='te', dest='en').text\n",
    "    except Exception as e:\n",
    "        en = f\"[translation error: {e}]\"\n",
    "    print(f\"{w}\\tcount={c}\\t→ {en}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287a0a3d",
   "metadata": {},
   "source": [
    "### We conclude the words are \"stop words\" that we learned in the lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4a95d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Language  Total  Answerable  Unanswerable  Answerable Ratio\n",
      "train       ar   2558        2303           255          0.900313\n",
      "train       ko   2422        2359            63          0.973988\n",
      "train       te   1355        1310            45          0.966790\n",
      "  val       ar    415         363            52          0.874699\n",
      "  val       ko    356         337            19          0.946629\n",
      "  val       te    384         291            93          0.757812\n"
     ]
    }
   ],
   "source": [
    "# Stats about answerable vs unanswerable questions\n",
    "\n",
    "# Define languages and splits\n",
    "\n",
    "split_dfs = {\n",
    "    \"train\": df_train,\n",
    "    \"val\":   df_val\n",
    "}\n",
    "\n",
    "\n",
    "rows = []\n",
    "for split_name, df in split_dfs.items():\n",
    "    for lang in langs:\n",
    "        total = df[df[\"lang\"] == lang].shape[0]\n",
    "        ans   = df[(df[\"lang\"] == lang) & (df[\"answerable\"])].shape[0]\n",
    "        unans = total - ans\n",
    "        ratio = ans / total if total > 0 else 0\n",
    "        rows.append([split_name, lang, total, ans, unans, ratio])\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary = pd.DataFrame(rows, columns=[\"Split\", \"Language\", \"Total\", \"Answerable\", \"Unanswerable\", \"Answerable Ratio\"])\n",
    "print(summary.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5a706b",
   "metadata": {},
   "source": [
    "## RULE BASE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ba3e4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_cache = {}\n",
    "\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "def translate_to_en_cached(texts, src_lang, batch_size=50):\n",
    "    out = []\n",
    "    new_texts = [t for t in texts if t not in translation_cache]\n",
    "\n",
    "    for i in range(0, len(new_texts), batch_size):\n",
    "        batch = new_texts[i:i+batch_size]\n",
    "        try:\n",
    "            translations = translator.translate(batch, src=src_lang, dest='en')\n",
    "            for t, trans in zip(batch, translations):\n",
    "                translation_cache[t] = trans.text\n",
    "        except Exception:\n",
    "            for t in batch:\n",
    "                translation_cache[t] = \"\"  \n",
    "\n",
    "    \n",
    "    out = [translation_cache[t] for t in texts]\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b778d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hadn', 'these', 'if', 'below', 'isn', \"should've\", 'my', 'its', 'too', \"she'll\", \"i'm\", 'those', 'myself', 'both', 'themselves', 'himself', \"shouldn't\", \"it'll\", 'won', \"you've\", '%', \"it'd\", 'being', \"couldn't\", 'an', \"you'll\", 're', 'any', 'a', 'does', '_', \"it's\", 'some', \"didn't\", \"isn't\", 'off', \"they'll\", 'each', \"i've\", 'wasn', 'at', 'him', 'how', 'to', '$', \"i'd\", 'haven', 'hers', 'll', '@', 'because', 'needn', 'until', 'yourself', 'is', \"needn't\", \"you're\", 'ma', 'over', \"they'd\", 'this', 'been', 'through', '/', 'be', 'can', 'hasn', 'having', 'down', 'own', '[', \"doesn't\", 'but', 'doing', \"he'll\", 'do', ']', 'we', 'why', 'out', 'under', 'you', ':', 'ourselves', '.', 'm', 'have', 'between', \"aren't\", 'on', 'only', 'yours', '!', '`', 'your', 'as', 'with', 'all', '&', 'couldn', 'where', 'once', \"you'd\", 'shan', 'what', 'has', 'i', \"mustn't\", 'for', '>', '\\\\', \"haven't\", \"'\", 'she', 'again', 'y', 'their', \"she'd\", \"he'd\", 'before', 'just', 'weren', 'itself', 'while', 'which', 'by', 'don', ')', 'ain', 'into', 'here', 'did', 'when', 'that', 'during', 'not', 'them', '{', 'he', 'o', 't', \"wasn't\", 'after', 'were', 've', '^', 'most', '|', 'it', 'now', 'am', 'in', 'so', 'there', ',', '~', 'same', \"don't\", 'no', 'than', 'then', 'they', ';', 'our', '?', 'few', 'herself', 'was', 'aren', 'against', 'nor', \"that'll\", 'from', 'will', 'above', 'or', \"hadn't\", 'had', 'the', \"we've\", \"wouldn't\", 'very', 'of', \"they're\", '-', '\"', 'didn', 'd', 'further', 'shouldn', 'such', \"she's\", 'up', 'me', 's', '#', 'his', 'are', \"hasn't\", \"weren't\", \"won't\", 'doesn', 'mightn', 'yourselves', \"i'll\", 'about', '=', 'ours', 'other', \"he's\", 'her', \"shan't\", \"they've\", \"we're\", \"we'll\", \"we'd\", '*', 'whom', \"mightn't\", '(', 'and', 'who', 'wouldn', 'theirs', 'more', '<', '}', 'should', '+', 'mustn'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#STOP WORDS\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) | set(string.punctuation)\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "452f6271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "[ar] emptyQ train/val: 100.00% / 100.00%\n",
      "[ko] emptyQ train/val: 100.00% / 100.00%\n",
      "[te] emptyQ train/val: 100.00% / 100.00%\n",
      "\n",
      " lang  threshold  train_F1_at_T  val_acc  val_prec  val_rec  val_f1\n",
      "  ar        0.0         0.9475   0.8747    0.8747      1.0  0.9332\n",
      "  ko        0.0         0.9868   0.9466    0.9466      1.0  0.9726\n",
      "  te        0.0         0.9831   0.7578    0.7578      1.0  0.8622\n",
      "\n",
      "Confusion matrices:\n",
      "ar {'TP': 363, 'FP': 52, 'FN': 0, 'TN': 0}\n",
      "ko {'TP': 337, 'FP': 19, 'FN': 0, 'TN': 0}\n",
      "te {'TP': 291, 'FP': 93, 'FN': 0, 'TN': 0}\n"
     ]
    }
   ],
   "source": [
    "# ================== INSTALLS (run once if needed) ==================\n",
    "# !pip install -q transformers sentencepiece sacremoses torch nltk regex unidecode\n",
    "\n",
    "# ================== IMPORTS & SETUP ==================\n",
    "import numpy as np, pandas as pd, regex as re, string\n",
    "from unidecode import unidecode\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "EN_STOP = set(stopwords.words('english')) | set(string.punctuation)\n",
    "\n",
    "# ================== NLLB-200 (GPU) ==================\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "MODEL_ID = \"facebook/nllb-200-distilled-600M\"\n",
    "SRC_CODES = {\"ar\":\"arb_Arab\", \"ko\":\"kor_Hang\", \"te\":\"tel_Telu\"}\n",
    "TGT_CODE = \"eng_Latn\"\n",
    "\n",
    "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device_str)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "mt  = AutoModelForSeq2SeqLM.from_pretrained(MODEL_ID).to(device_str).eval()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def translate_list(texts, src_lang_code, batch_size=32, max_length=320):\n",
    "    out = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = [str(t) for t in texts[i:i+batch_size]]\n",
    "        try:\n",
    "            enc = tok(batch, return_tensors=\"pt\", padding=True, truncation=True,\n",
    "                      max_length=max_length, src_lang=src_lang_code).to(device_str)\n",
    "            gen = mt.generate(**enc, forced_bos_token_id=tok.lang_code_to_id[TGT_CODE],\n",
    "                              max_length=max_length)\n",
    "            dec = tok.batch_decode(gen, skip_special_tokens=True)\n",
    "        except Exception:\n",
    "            dec = [\"\"] * len(batch)\n",
    "        out.extend(dec)\n",
    "    return out\n",
    "\n",
    "# ================== NLTK tokenize + stopword removal ==================\n",
    "def tok_en_rm_stop(text: str):\n",
    "    if not isinstance(text, str): return []\n",
    "    toks = word_tokenize(text, language=\"english\", preserve_line=True)  # avoids punkt model\n",
    "    return [t.lower() for t in toks if t and t.lower() not in EN_STOP]\n",
    "\n",
    "# ================== Feature builders ==================\n",
    "ALNUM = re.compile(r\"[a-z0-9]+\")\n",
    "def ascii_alnum(s):\n",
    "    s2 = unidecode((s or \"\").lower())\n",
    "    return \" \".join(ALNUM.findall(s2))\n",
    "\n",
    "def char_ngrams(s, n=3):\n",
    "    s = s.replace(\" \", \"\")\n",
    "    return [s[i:i+n] for i in range(max(0, len(s)-n+1))]\n",
    "\n",
    "NUM_RE = re.compile(r\"\\d{2,}\")  # years/counts\n",
    "\n",
    "def build_features(q_texts, c_texts, q_toks, c_toks):\n",
    "    F = []\n",
    "    for qx, cx, q, c in zip(q_texts, c_texts, q_toks, c_toks):\n",
    "        qs, cs = set(q), set(c)\n",
    "        inter = len(qs & cs)\n",
    "        overlap = inter / max(1, len(q))\n",
    "        jacc   = inter / max(1, len(qs | cs))\n",
    "\n",
    "        # number match on raw translated strings\n",
    "        q_nums = set(NUM_RE.findall(qx or \"\"))\n",
    "        has_num  = 1.0 if any(n in (cx or \"\") for n in q_nums) else 0.0\n",
    "\n",
    "        # long token anchor (>=6 chars) present in context tokens\n",
    "        has_long = 1.0 if any((len(t)>=6 and t in cs) for t in q) else 0.0\n",
    "\n",
    "        # char-3-gram overlap on raw ascii-folded strings\n",
    "        qa, ca = ascii_alnum(qx or \"\"), ascii_alnum(cx or \"\")\n",
    "        q3 = set(char_ngrams(qa, 3))\n",
    "        char3 = (sum(g in ca for g in q3)/len(q3)) if q3 else 0.0\n",
    "\n",
    "        F.append((overlap, jacc, has_num, has_long, char3))\n",
    "    return np.array(F, float)\n",
    "\n",
    "def score(F):\n",
    "    # [overlap, jaccard, num, long, char3]\n",
    "    w = np.array([0.35, 0.05, 0.20, 0.10, 0.30], float)\n",
    "    return (F*w).sum(axis=1)\n",
    "\n",
    "def best_threshold(scores, gold):\n",
    "    best_t, best_f1 = 0.0, -1.0\n",
    "    for T in np.linspace(0, 1, 101):\n",
    "        pred = scores >= T\n",
    "        tp = np.sum((pred==1)&(gold==1))\n",
    "        fp = np.sum((pred==1)&(gold==0))\n",
    "        fn = np.sum((pred==0)&(gold==1))\n",
    "        prec = tp/(tp+fp) if (tp+fp) else 0.0\n",
    "        rec  = tp/(tp+fn) if (tp+fn) else 0.0\n",
    "        f1   = 2*prec*rec/(prec+rec) if (prec+rec) else 0.0\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, T\n",
    "    return float(best_t), float(best_f1)\n",
    "\n",
    "# ================== Train→Val per-language evaluation ==================\n",
    "def eval_language(lang_code):\n",
    "    tr = df_train[df_train[\"lang\"]==lang_code].copy()\n",
    "    va = df_val[df_val[\"lang\"]==lang_code].copy()\n",
    "\n",
    "    # Translate Q & C to English\n",
    "    src = SRC_CODES[lang_code]\n",
    "    tr_q_en = translate_list(tr[\"question\"].astype(str).tolist(), src)\n",
    "    tr_c_en = translate_list(tr[\"context\"].astype(str).tolist(),  src)\n",
    "    va_q_en = translate_list(va[\"question\"].astype(str).tolist(), src)\n",
    "    va_c_en = translate_list(va[\"context\"].astype(str).tolist(),  src)\n",
    "\n",
    "    # Quick sanity check\n",
    "    pct_empty = lambda L: np.mean([(not isinstance(x,str)) or (x.strip()==\"\") for x in L])*100\n",
    "    print(f\"[{lang_code}] emptyQ train/val: {pct_empty(tr_q_en):.2f}% / {pct_empty(va_q_en):.2f}%\")\n",
    "\n",
    "    # Tokenize (NLTK) + remove English stops\n",
    "    tr_q_tok = [tok_en_rm_stop(s) for s in tr_q_en]\n",
    "    tr_c_tok = [tok_en_rm_stop(s) for s in tr_c_en]\n",
    "    va_q_tok = [tok_en_rm_stop(s) for s in va_q_en]\n",
    "    va_c_tok = [tok_en_rm_stop(s) for s in va_c_en]\n",
    "\n",
    "    # Features → scores\n",
    "    F_tr = build_features(tr_q_en, tr_c_en, tr_q_tok, tr_c_tok)\n",
    "    s_tr = score(F_tr)\n",
    "    T, f1_tr = best_threshold(s_tr, tr[\"answerable\"].astype(int).values)\n",
    "\n",
    "    F_va = build_features(va_q_en, va_c_en, va_q_tok, va_c_tok)\n",
    "    s_va = score(F_va)\n",
    "    pred = (s_va >= T).astype(int)\n",
    "    gold = va[\"answerable\"].astype(int).values\n",
    "\n",
    "    tp = int(((pred==1)&(gold==1)).sum())\n",
    "    fp = int(((pred==1)&(gold==0)).sum())\n",
    "    fn = int(((pred==0)&(gold==1)).sum())\n",
    "    tn = int(((pred==0)&(gold==0)).sum())\n",
    "\n",
    "    acc  = (tp+tn)/max(1, tp+tn+fp+fn)\n",
    "    prec = tp/max(1, tp+fp)\n",
    "    rec  = tp/max(1, tp+fn)\n",
    "    f1   = 0.0 if (prec+rec)==0 else 2*prec*rec/(prec+rec)\n",
    "\n",
    "    return {\n",
    "        \"lang\": lang_code,\n",
    "        \"threshold\": round(T,3),\n",
    "        \"train_F1_at_T\": round(f1_tr,4),\n",
    "        \"val_acc\": round(acc,4),\n",
    "        \"val_prec\": round(prec,4),\n",
    "        \"val_rec\": round(rec,4),\n",
    "        \"val_f1\": round(f1,4),\n",
    "        \"cm\": {\"TP\":tp,\"FP\":fp,\"FN\":fn,\"TN\":tn},\n",
    "    }\n",
    "\n",
    "# ===== Run for Arabic / Korean / Telugu =====\n",
    "results = [eval_language(l) for l in [\"ar\",\"ko\",\"te\"]]\n",
    "summary = pd.DataFrame([{k:v for k,v in r.items() if k!=\"cm\"} for r in results])\n",
    "print(\"\\n\", summary.to_string(index=False))\n",
    "print(\"\\nConfusion matrices:\")\n",
    "for r in results:\n",
    "    print(r[\"lang\"], r[\"cm\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24aaf896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ar] emptyQ train: 100.00% | emptyC train: 100.00%\n",
      "Score stats (train): min 0.0 max 0.0 mean 0.0\n",
      "Frac > 0: 0.0\n",
      "Feature mins: {'overlap': np.float64(0.0), 'jacc': np.float64(0.0), 'has_num': np.float64(0.0), 'has_long': np.float64(0.0), 'char3': np.float64(0.0)}\n",
      "Feature maxs: {'overlap': np.float64(0.0), 'jacc': np.float64(0.0), 'has_num': np.float64(0.0), 'has_long': np.float64(0.0), 'char3': np.float64(0.0)}\n",
      "Feature means: {'overlap': np.float64(0.0), 'jacc': np.float64(0.0), 'has_num': np.float64(0.0), 'has_long': np.float64(0.0), 'char3': np.float64(0.0)}\n",
      "\n",
      "Sample translations:\n",
      "Q_SRC: متى تدخلت روسيا في  الحرب الأهلية السورية؟\n",
      "Q_EN : \n",
      "C_EN (first 100): \n",
      "---\n",
      "Q_SRC: متى حصلت هنغاريا على استقلالها من النمسا ؟\n",
      "Q_EN : \n",
      "C_EN (first 100): \n",
      "---\n",
      "Q_SRC: متى تحالفت فرنسا و بريطانيا العظمى ضد ألمانيا في حرب؟\n",
      "Q_EN : \n",
      "C_EN (first 100): \n",
      "---\n",
      "\n",
      "[ko] emptyQ train: 100.00% | emptyC train: 100.00%\n",
      "Score stats (train): min 0.0 max 0.0 mean 0.0\n",
      "Frac > 0: 0.0\n",
      "Feature mins: {'overlap': np.float64(0.0), 'jacc': np.float64(0.0), 'has_num': np.float64(0.0), 'has_long': np.float64(0.0), 'char3': np.float64(0.0)}\n",
      "Feature maxs: {'overlap': np.float64(0.0), 'jacc': np.float64(0.0), 'has_num': np.float64(0.0), 'has_long': np.float64(0.0), 'char3': np.float64(0.0)}\n",
      "Feature means: {'overlap': np.float64(0.0), 'jacc': np.float64(0.0), 'has_num': np.float64(0.0), 'has_long': np.float64(0.0), 'char3': np.float64(0.0)}\n",
      "\n",
      "Sample translations:\n",
      "Q_SRC: 30년 전쟁의 승자는 누구인가?\n",
      "Q_EN : \n",
      "C_EN (first 100): \n",
      "---\n",
      "Q_SRC: 엑스선은 누가 발견하였는가?\n",
      "Q_EN : \n",
      "C_EN (first 100): \n",
      "---\n",
      "Q_SRC: 아테네에서 언제 가장 최근의 올림픽이 올렸나요?\n",
      "Q_EN : \n",
      "C_EN (first 100): \n",
      "---\n",
      "\n",
      "[te] emptyQ train: 100.00% | emptyC train: 100.00%\n",
      "Score stats (train): min 0.0 max 0.0 mean 0.0\n",
      "Frac > 0: 0.0\n",
      "Feature mins: {'overlap': np.float64(0.0), 'jacc': np.float64(0.0), 'has_num': np.float64(0.0), 'has_long': np.float64(0.0), 'char3': np.float64(0.0)}\n",
      "Feature maxs: {'overlap': np.float64(0.0), 'jacc': np.float64(0.0), 'has_num': np.float64(0.0), 'has_long': np.float64(0.0), 'char3': np.float64(0.0)}\n",
      "Feature means: {'overlap': np.float64(0.0), 'jacc': np.float64(0.0), 'has_num': np.float64(0.0), 'has_long': np.float64(0.0), 'char3': np.float64(0.0)}\n",
      "\n",
      "Sample translations:\n",
      "Q_SRC: ప్రపంచంలో  మొట్టమొదటి దూర విద్య విద్యాలయం ఏ దేశంలో స్థాపించబడింది ?\n",
      "Q_EN : \n",
      "C_EN (first 100): \n",
      "---\n",
      "Q_SRC: 1959వ సంవత్సరంలో భారతదేశ ప్రధాన మంత్రి  ఎవరు?\n",
      "Q_EN : \n",
      "C_EN (first 100): \n",
      "---\n",
      "Q_SRC: ఏ కాకతీయ రాజు కర్నూలు జిల్లాను చివరిగా పాలించాడు?\n",
      "Q_EN : \n",
      "C_EN (first 100): \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# ================== DIAGNOSTICS CELL ==================\n",
    "\n",
    "def debug_language(lang_code):\n",
    "    tr = df_train[df_train[\"lang\"]==lang_code].copy()\n",
    "    src = SRC_CODES[lang_code]\n",
    "\n",
    "    # Translate Q & C to English\n",
    "    tr_q_en = translate_list(tr[\"question\"].astype(str).tolist(), src)\n",
    "    tr_c_en = translate_list(tr[\"context\"].astype(str).tolist(), src)\n",
    "\n",
    "    # Sanity: empty translation %\n",
    "    def pct_empty(lst): \n",
    "        return np.mean([(not isinstance(x,str)) or (x.strip()==\"\") for x in lst])*100\n",
    "    print(f\"\\n[{lang_code}] emptyQ train: {pct_empty(tr_q_en):.2f}% | emptyC train: {pct_empty(tr_c_en):.2f}%\")\n",
    "\n",
    "    # Tokenize (with current settings)\n",
    "    tr_q_tok = [tok_en_rm_stop(s) for s in tr_q_en]\n",
    "    tr_c_tok = [tok_en_rm_stop(s) for s in tr_c_en]\n",
    "\n",
    "    # Features → scores\n",
    "    F_tr = build_features(tr_q_en, tr_c_en, tr_q_tok, tr_c_tok)\n",
    "    s_tr = score(F_tr)\n",
    "\n",
    "    # Print stats\n",
    "    print(\"Score stats (train): min\", s_tr.min(), \"max\", s_tr.max(), \"mean\", s_tr.mean())\n",
    "    print(\"Frac > 0:\", (s_tr > 0).mean())\n",
    "\n",
    "    names = [\"overlap\",\"jacc\",\"has_num\",\"has_long\",\"char3\"]\n",
    "    print(\"Feature mins:\", dict(zip(names, F_tr.min(axis=0))))\n",
    "    print(\"Feature maxs:\", dict(zip(names, F_tr.max(axis=0))))\n",
    "    print(\"Feature means:\",dict(zip(names, F_tr.mean(axis=0))))\n",
    "\n",
    "    # Print sample translations\n",
    "    print(\"\\nSample translations:\")\n",
    "    for i in range(min(3, len(tr_q_en))):\n",
    "        print(f\"Q_SRC: {tr['question'].iloc[i]}\")\n",
    "        print(f\"Q_EN : {tr_q_en[i]}\")\n",
    "        print(f\"C_EN (first 100): {(tr_c_en[i] or '')[:100]}\")\n",
    "        print(\"---\")\n",
    "\n",
    "# Run for each language\n",
    "for l in [\"ar\",\"ko\",\"te\"]:\n",
    "    debug_language(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1dd6d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0f125a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ================== IMPORTS & SETUP ==================\n",
    "import numpy as np, pandas as pd, regex as re, string\n",
    "from unidecode import unidecode\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "EN_STOP = set(stopwords.words('english')) | set(string.punctuation)\n",
    "\n",
    "# ================== NLLB-200 (GPU) ==================\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "MODEL_ID = \"facebook/nllb-200-distilled-600M\"\n",
    "SRC_CODES = {\"ar\":\"arb_Arab\", \"ko\":\"kor_Hang\", \"te\":\"tel_Telu\"}\n",
    "TGT_CODE = \"eng_Latn\"\n",
    "\n",
    "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17b1da5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.7.1+cu118\n",
      "CUDA available: True\n",
      "Using device idx for HF pipeline: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Smoke test [ar]\n",
      "SRC: مرحبا\n",
      "EN : Hey , what 's up ?\n",
      "SRC: متى تأسست جامعة القاهرة؟\n",
      "EN : When was Cairo University founded?\n",
      "\n",
      "Smoke test [ko]\n",
      "SRC: 안녕하세요\n",
      "EN : Hey, what's up?\n",
      "SRC: 서울의 인구는 얼마입니까?\n",
      "EN : What is the population of Seoul?\n",
      "\n",
      "Smoke test [te]\n",
      "SRC: నమస్కారం\n",
      "EN : Greetings\n",
      "SRC: హైదరాబాదు ఎక్కడ ఉంది?\n",
      "EN : Where is Hyderabad?\n",
      "\n",
      "[ar] sample translation empties (questions): 0.00%  on 16 items\n",
      "Q_SRC: متى تدخلت روسيا في  الحرب الأهلية السورية؟\n",
      "Q_EN : When did Russia intervene in the Syrian civil war?\n",
      "---\n",
      "Q_SRC: متى حصلت هنغاريا على استقلالها من النمسا ؟\n",
      "Q_EN : When did Hungary gain its independence from Austria ?\n",
      "---\n",
      "\n",
      "[ko] sample translation empties (questions): 0.00%  on 16 items\n",
      "Q_SRC: 30년 전쟁의 승자는 누구인가?\n",
      "Q_EN : Who is the winner of the Thirty Years' War?\n",
      "---\n",
      "Q_SRC: 엑스선은 누가 발견하였는가?\n",
      "Q_EN : Who discovered X-rays?\n",
      "---\n",
      "\n",
      "[te] sample translation empties (questions): 0.00%  on 16 items\n",
      "Q_SRC: ప్రపంచంలో  మొట్టమొదటి దూర విద్య విద్యాలయం ఏ దేశంలో స్థాపించబడింది ?\n",
      "Q_EN : The world's first distance learning university was established in which country?\n",
      "---\n",
      "Q_SRC: 1959వ సంవత్సరంలో భారతదేశ ప్రధాన మంత్రి  ఎవరు?\n",
      "Q_EN : Who was the Prime Minister of India in 1959?\n",
      "---\n",
      "\n",
      "[Info] Skipping context translation (contexts are already English in this dataset).\n"
     ]
    }
   ],
   "source": [
    "# ===== FIX & VERIFY TRANSLATION (NLLB-200) =====\n",
    "# If you just installed CUDA/PyTorch, RESTART kernel before running this.\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(\"Using device idx for HF pipeline:\", device)\n",
    "\n",
    "MODEL_ID = \"facebook/nllb-200-distilled-600M\"\n",
    "SRC_CODES = {\"ar\": \"arb_Arab\", \"ko\": \"kor_Hang\", \"te\": \"tel_Telu\"}\n",
    "TGT_CODE = \"eng_Latn\"\n",
    "\n",
    "# Build a single multilingual translation pipeline\n",
    "nllb = pipeline(\"translation\", model=MODEL_ID, tokenizer=MODEL_ID, device=device)\n",
    "\n",
    "def translate_list_pipe(texts, src_lang, batch_size=32, max_length=320):\n",
    "    \"\"\"Robust batched translation via HF pipeline, with explicit src/tgt codes and no silent failures.\"\"\"\n",
    "    out = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = [str(x) if isinstance(x, str) else \"\" for x in texts[i:i+batch_size]]\n",
    "        try:\n",
    "            preds = nllb(batch, src_lang=SRC_CODES[src_lang], tgt_lang=TGT_CODE,\n",
    "                         truncation=True, max_length=max_length)\n",
    "            out.extend([p[\"translation_text\"] for p in preds])\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Translation failed for batch {i}:{i+len(batch)} — {type(e).__name__}: {e}\")\n",
    "            out.extend([\"\"] * len(batch))\n",
    "    return out\n",
    "\n",
    "def pct_empty(lst):\n",
    "    return np.mean([(not isinstance(x,str)) or (x.strip()==\"\") for x in lst]) * 100.0\n",
    "\n",
    "# --- 1) Smoke-test the model with known strings (should NOT be empty) ---\n",
    "tests = {\n",
    "    \"ar\": [\"مرحبا\", \"متى تأسست جامعة القاهرة؟\"],\n",
    "    \"ko\": [\"안녕하세요\", \"서울의 인구는 얼마입니까?\"],\n",
    "    \"te\": [\"నమస్కారం\", \"హైదరాబాదు ఎక్కడ ఉంది?\"],\n",
    "}\n",
    "for lg, arr in tests.items():\n",
    "    tr = translate_list_pipe(arr, lg, batch_size=2)\n",
    "    print(f\"\\nSmoke test [{lg}]\")\n",
    "    for src, tgt in zip(arr, tr):\n",
    "        print(\"SRC:\", src)\n",
    "        print(\"EN :\", tgt)\n",
    "\n",
    "# --- 2) Sanity-check a small slice from your dataset (questions) ---\n",
    "SAMPLE = 16  # small to be fast; raise after it works\n",
    "for lg in [\"ar\",\"ko\",\"te\"]:\n",
    "    q_src = df_train.loc[df_train[\"lang\"]==lg, \"question\"].astype(str).head(SAMPLE).tolist()\n",
    "    q_en  = translate_list_pipe(q_src, lg, batch_size=8)\n",
    "    print(f\"\\n[{lg}] sample translation empties (questions): {pct_empty(q_en):.2f}%  on {len(q_en)} items\")\n",
    "    # peek a couple\n",
    "    for i in range(min(2, len(q_en))):\n",
    "        print(\"Q_SRC:\", q_src[i])\n",
    "        print(\"Q_EN :\", q_en[i])\n",
    "        print(\"---\")\n",
    "\n",
    "# --- 3) Contexts in TyDi XOR RC are already English; translating them is unnecessary & slow.\n",
    "#       If your assignment requires translating both, you can do it — but we recommend skipping for speed.\n",
    "TRANSLATE_CONTEXT = False  # set True only if you must translate contexts too\n",
    "\n",
    "if TRANSLATE_CONTEXT:\n",
    "    for lg in [\"ar\",\"ko\",\"te\"]:\n",
    "        c_src = df_train.loc[df_train[\"lang\"]==lg, \"context\"].astype(str).head(SAMPLE).tolist()\n",
    "        c_en  = translate_list_pipe(c_src, lg, batch_size=8)\n",
    "        print(f\"\\n[{lg}] sample translation empties (contexts): {pct_empty(c_en):.2f}%  on {len(c_en)} items\")\n",
    "        for i in range(min(1, len(c_en))):\n",
    "            print(\"C_SRC (first 150):\", c_src[i][:150])\n",
    "            print(\"C_EN  (first 150):\", c_en[i][:150])\n",
    "            print(\"---\")\n",
    "else:\n",
    "    print(\"\\n[Info] Skipping context translation (contexts are already English in this dataset).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765891c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "# ================== RULE-BASED CLASSIFIER (with CUDA NLLB Q-translation) ==================\n",
    "import numpy as np, pandas as pd, regex as re, string\n",
    "from unidecode import unidecode\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "EN_STOP = set(stopwords.words('english')) | set(string.punctuation)\n",
    "\n",
    "# ---- tokenization helpers ----\n",
    "def tok_en_rm_stop(text: str):\n",
    "    if not isinstance(text, str): return []\n",
    "    toks = word_tokenize(text, language=\"english\", preserve_line=True)\n",
    "    return [t.lower() for t in toks if t and t.lower() not in EN_STOP]\n",
    "\n",
    "def tok_en_keep_all(text: str):\n",
    "    if not isinstance(text, str): return []\n",
    "    toks = word_tokenize(text, language=\"english\", preserve_line=True)\n",
    "    return [t.lower() for t in toks if t]\n",
    "\n",
    "# ---- text normalization / features ----\n",
    "ALNUM = re.compile(r\"[a-z0-9]+\")\n",
    "def ascii_alnum(s):\n",
    "    s2 = unidecode((s or \"\").lower())\n",
    "    return \" \".join(ALNUM.findall(s2))\n",
    "\n",
    "def char_ngrams(s, n=3):\n",
    "    s = s.replace(\" \", \"\")\n",
    "    return [s[i:i+n] for i in range(max(0, len(s)-n+1))]\n",
    "\n",
    "NUM_RE = re.compile(r\"\\d{2,}\")  # match 2+ digit numbers (years, counts)\n",
    "\n",
    "def build_features(q_texts, c_texts, q_toks, c_toks):\n",
    "    \"\"\"\n",
    "    Return Nx5 features: [overlap, jaccard, has_num, has_long, char3]\n",
    "    \"\"\"\n",
    "    F = []\n",
    "    for qx, cx, q, c in zip(q_texts, c_texts, q_toks, c_toks):\n",
    "        qs, cs = set(q), set(c)\n",
    "        inter = len(qs & cs)\n",
    "        overlap = inter / max(1, len(q))\n",
    "        jacc   = inter / max(1, len(qs | cs))\n",
    "\n",
    "        # numbers on RAW strings\n",
    "        q_nums = set(NUM_RE.findall(qx or \"\"))\n",
    "        has_num  = 1.0 if any(n in (cx or \"\") for n in q_nums) else 0.0\n",
    "\n",
    "        # long token anchor (>=5 chars) present in context tokens\n",
    "        has_long = 1.0 if any((len(t) >= 5 and t in cs) for t in q) else 0.0\n",
    "\n",
    "        # char-3-gram overlap on ascii-folded RAW strings\n",
    "        qa, ca = ascii_alnum(qx or \"\"), ascii_alnum(cx or \"\")\n",
    "        q3 = set(char_ngrams(qa, 3))\n",
    "        char3 = (sum(g in ca for g in q3)/len(q3)) if q3 else 0.0\n",
    "\n",
    "        F.append((overlap, jacc, has_num, has_long, char3))\n",
    "    return np.array(F, float)\n",
    "\n",
    "def score(F):\n",
    "    # weights: emphasize token overlap + char3; support from numbers/long; small jaccard\n",
    "    w = np.array([0.40, 0.05, 0.20, 0.10, 0.25], float)\n",
    "    return (F*w).sum(axis=1)\n",
    "\n",
    "def best_threshold(scores, gold):\n",
    "    best_t, best_f1 = 0.0, -1.0\n",
    "    for T in np.linspace(0, 1, 101):\n",
    "        pred = scores >= T\n",
    "        tp = np.sum((pred==1)&(gold==1))\n",
    "        fp = np.sum((pred==1)&(gold==0))\n",
    "        fn = np.sum((pred==0)&(gold==1))\n",
    "        prec = tp/(tp+fp) if (tp+fp) else 0.0\n",
    "        rec  = tp/(tp+fn) if (tp+fn) else 0.0\n",
    "        f1   = 2*prec*rec/(prec+rec) if (prec+rec) else 0.0\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, T\n",
    "    return float(best_t), float(best_f1)\n",
    "\n",
    "def eval_language_rule_based(lang_code, translate_contexts=False, batch_size=64):\n",
    "    # 1) slice data\n",
    "    tr = df_train[df_train[\"lang\"]==lang_code].copy()\n",
    "    va = df_val[df_val[\"lang\"]==lang_code].copy()\n",
    "\n",
    "    # 2) translate questions → EN (uses working nllb+translate_list_pipe from previous cell)\n",
    "    tr_q_en = translate_list_pipe(tr[\"question\"].astype(str).tolist(), lang_code, batch_size=batch_size)\n",
    "    va_q_en = translate_list_pipe(va[\"question\"].astype(str).tolist(), lang_code, batch_size=batch_size)\n",
    "\n",
    "    # 3) contexts: either keep as-is (English already) or translate if you must\n",
    "    if translate_contexts:\n",
    "        tr_c_en = translate_list_pipe(tr[\"context\"].astype(str).tolist(), lang_code, batch_size=batch_size)\n",
    "        va_c_en = translate_list_pipe(va[\"context\"].astype(str).tolist(), lang_code, batch_size=batch_size)\n",
    "    else:\n",
    "        tr_c_en = tr[\"context\"].astype(str).tolist()\n",
    "        va_c_en = va[\"context\"].astype(str).tolist()\n",
    "\n",
    "    # 4) tokenize: remove stops in Q, keep all tokens in C (better recall)\n",
    "    tr_q_tok = [tok_en_rm_stop(s) for s in tr_q_en]\n",
    "    va_q_tok = [tok_en_rm_stop(s) for s in va_q_en]\n",
    "    tr_c_tok = [tok_en_keep_all(s) for s in tr_c_en]\n",
    "    va_c_tok = [tok_en_keep_all(s) for s in va_c_en]\n",
    "\n",
    "    # 5) features → scores → threshold on train\n",
    "    F_tr = build_features(tr_q_en, tr_c_en, tr_q_tok, tr_c_tok)\n",
    "    s_tr = score(F_tr)\n",
    "    T, f1_tr = best_threshold(s_tr, tr[\"answerable\"].astype(int).values)\n",
    "\n",
    "    # 6) eval on val\n",
    "    F_va = build_features(va_q_en, va_c_en, va_q_tok, va_c_tok)\n",
    "    s_va = score(F_va)\n",
    "    pred = (s_va >= T).astype(int)\n",
    "    gold = va[\"answerable\"].astype(int).values\n",
    "\n",
    "    tp = int(((pred==1)&(gold==1)).sum())\n",
    "    fp = int(((pred==1)&(gold==0)).sum())\n",
    "    fn = int(((pred==0)&(gold==1)).sum())\n",
    "    tn = int(((pred==0)&(gold==0)).sum())\n",
    "\n",
    "    acc  = (tp+tn)/max(1, tp+tn+fp+fn)\n",
    "    prec = tp/max(1, tp+fp)\n",
    "    rec  = tp/max(1, tp+fn)\n",
    "    f1   = 0.0 if (prec+rec)==0 else 2*prec*rec/(prec+rec)\n",
    "\n",
    "    return {\n",
    "        \"lang\": lang_code,\n",
    "        \"threshold\": round(T,3),\n",
    "        \"train_F1_at_T\": round(f1_tr,4),\n",
    "        \"val_acc\": round(acc,4),\n",
    "        \"val_prec\": round(prec,4),\n",
    "        \"val_rec\": round(rec,4),\n",
    "        \"val_f1\": round(f1,4),\n",
    "        \"cm\": {\"TP\":tp,\"FP\":fp,\"FN\":fn,\"TN\":tn},\n",
    "        \"score_min_max_train\": (float(s_tr.min()), float(s_tr.max())),\n",
    "        \"score_min_max_val\": (float(s_va.min()), float(s_va.max())),\n",
    "    }\n",
    "\n",
    "# ---- run for all three languages\n",
    "results = [eval_language_rule_based(l, translate_contexts=False, batch_size=64) for l in [\"ar\",\"ko\",\"te\"]]\n",
    "\n",
    "summary = pd.DataFrame([{k:v for k,v in r.items() if k not in (\"cm\",\"score_min_max_train\",\"score_min_max_val\")} for r in results])\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "print(\"\\nConfusion matrices:\")\n",
    "for r in results:\n",
    "    print(r[\"lang\"], r[\"cm\"])\n",
    "\n",
    "print(\"\\nScore ranges:\")\n",
    "for r in results:\n",
    "    print(r[\"lang\"], \"train\", r[\"score_min_max_train\"], \"| val\", r[\"score_min_max_val\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
