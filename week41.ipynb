{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70a3cd68-246c-4cb6-8e5f-cd9a4d3bcfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTS\n",
    "from collections import Counter\n",
    "from googletrans import Translator\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "from typing import List, Iterable, Dict, Optional\n",
    "from unidecode import unidecode\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    pipeline,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    T5Tokenizer,\n",
    "    MT5ForConditionalGeneration)\n",
    "\n",
    "import evaluate\n",
    "import sacrebleu\n",
    "import rouge_score\n",
    "import torch\n",
    "import pickle\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca71776-7c20-432b-b0e8-69399bffea52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea627e01-e582-4ce8-ac1d-1beeaa95fe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "langs  = [\"ko\"]\n",
    "lang = langs[0]\n",
    "splits = {'train': 'train.parquet', 'validation': 'validation.parquet', 'test': 'test.parquet'}\n",
    "df_train = pd.read_parquet(\"hf://datasets/coastalcph/tydi_xor_rc/\" + splits[\"train\"])\n",
    "df_val   = pd.read_parquet(\"hf://datasets/coastalcph/tydi_xor_rc/\" + splits[\"validation\"])\n",
    "df_test = pd.read_json('week41qa.json') # otherwise 'week41qa - simple.json'\n",
    " \n",
    "df_train = df_train[df_train.lang.isin(langs)].reset_index(drop=True)\n",
    "df_val   = df_val[df_val.lang.isin(langs)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "df_train = pd.concat((df_train,df_val))\n",
    "train_ds = Dataset.from_pandas(df_train[[\"lang\",\"question\",\"context\",\"answerable\",\"answer_start\",\"answer\"]], preserve_index=False)\n",
    "test_ds  = Dataset.from_pandas(df_test[ [\"lang\",\"question\",\"context\",\"answerable\",\"answer_start\",\"answer\"]], preserve_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455b53ce-b4fa-4d52-a3a4-8b15538cd43b",
   "metadata": {},
   "source": [
    "### Week 36 (Part 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfd9929-089e-4447-8b97-490ebcfb2338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adab86ea-df09-46b0-bad6-d64463202a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korean — TRAIN punctuation (char -> count):\n",
      "[('?', 2776), (',', 26), ('.', 25), ('-', 6), (\"'\", 6), ('\"', 6), (':', 2), ('/', 1), ('\\\\', 1), ('(', 1), (')', 1)]\n",
      "Korean — TEST punctuation (char -> count):\n",
      "[('?', 30), ('《', 1), ('》', 1)]\n",
      "\n",
      "Korean — TRAIN total words: 13594\n",
      "Korean — TEST total words: 131\n",
      "\n",
      "Korean — numeric tokens (train): 10\n",
      "Korean — numeric tokens (test): 0\n",
      "\n",
      "Korean — hyphenated tokens (train): 6\n",
      "Korean — hyphenated tokens (test): 0\n"
     ]
    }
   ],
   "source": [
    "## Each language total words (not counting punctuation)\n",
    "# tokenizer: split on \\W+ (non-word chars); protect hyphens between letters/digits\n",
    "# safeguard: build punctuation set from training+validation data, do not count these tokens as well\n",
    "ko_train_q = df_train[df_train[\"lang\"] == lang][\"question\"].astype(str)\n",
    "ko_test_q   = df_test[df_test[\"lang\"] == lang][\"question\"].astype(str)\n",
    "\n",
    "PUNCT_RE = re.compile(r\"\\p{P}\", re.UNICODE)\n",
    "SPLIT_RE = re.compile(r\"\\W+\", re.UNICODE)          # tokenizer\n",
    "\n",
    "\n",
    "ko_train_punct = Counter(ch for q in ko_train_q for ch in PUNCT_RE.findall(q))\n",
    "ko_test_punct   = Counter(ch for q in ko_test_q for ch in PUNCT_RE.findall(q))\n",
    "\n",
    "print(\"Korean — TRAIN punctuation (char -> count):\")\n",
    "print(ko_train_punct.most_common())\n",
    "print(\"Korean — TEST punctuation (char -> count):\")\n",
    "print(ko_test_punct.most_common())\n",
    "\n",
    "HY = \"HYPHENJOIN\" # placeholder for protected hyphens\n",
    "PROTECT_HYPHEN = re.compile(r\"(?<=[\\p{L}\\p{N}])-(?=[\\p{L}\\p{N}])\", re.UNICODE)  # hyphen between letters/digits\n",
    "\n",
    "# KOREAN\n",
    "ko_train_q = df_train[df_train[\"lang\"] == lang][\"question\"].astype(str)\n",
    "ko_test_q   = df_test[df_test[\"lang\"] == lang][\"question\"].astype(str)\n",
    "\n",
    "ko_punct_set = set(ch for q in pd.concat([ko_train_q, ko_test_q]) for ch in PUNCT_RE.findall(q))\n",
    "\n",
    "ko_train_tokens = []\n",
    "for q in ko_train_q:\n",
    "    q2 = PROTECT_HYPHEN.sub(HY, q)\n",
    "    toks = [t.replace(HY, \"-\") for t in SPLIT_RE.split(q2) if t and t not in ko_punct_set]\n",
    "    ko_train_tokens.extend(toks)\n",
    "\n",
    "ko_test_tokens = []\n",
    "for q in ko_test_q:\n",
    "    q2 = PROTECT_HYPHEN.sub(HY, q)\n",
    "    toks = [t.replace(HY, \"-\") for t in SPLIT_RE.split(q2) if t and t not in ko_punct_set]\n",
    "    ko_test_tokens.extend(toks)\n",
    "\n",
    "print()\n",
    "print(\"Korean — TRAIN total words:\", len(ko_train_tokens))\n",
    "print(\"Korean — TEST total words:\",   len(ko_test_tokens))\n",
    "\n",
    "\n",
    "ko_numbers_train = sum(1 for t in ko_train_tokens if t.isdigit())\n",
    "ko_numbers_test   = sum(1 for t in ko_test_tokens if t.isdigit())\n",
    "\n",
    "ko_hyphen_train = sum(1 for t in ko_train_tokens if \"-\" in t)\n",
    "ko_hyphen_test   = sum(1 for t in ko_test_tokens if \"-\" in t)\n",
    "\n",
    "print()\n",
    "print(\"Korean — numeric tokens (train):\", ko_numbers_train)\n",
    "print(\"Korean — numeric tokens (test):\",   ko_numbers_test)\n",
    "print()\n",
    "print(\"Korean — hyphenated tokens (train):\", ko_hyphen_train)\n",
    "print(\"Korean — hyphenated tokens (test):\",   ko_hyphen_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "270dfa4b-d4f9-4729-b5bd-f4dc04972685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Language  Total  Answerable  Unanswerable  Answerable Ratio\n",
      "train       ko   2778        2696            82          0.970482\n",
      " test       ko     30          29             1          0.966667\n"
     ]
    }
   ],
   "source": [
    "# Stats about answerable vs unanswerable questions\n",
    "\n",
    "# Define languages and splits\n",
    "\n",
    "split_dfs = {\n",
    "    \"train\": df_train,\n",
    "    \"test\":  df_test\n",
    "}\n",
    "\n",
    "\n",
    "rows = []\n",
    "for split_name, df in split_dfs.items():\n",
    "    for lang in langs:\n",
    "        total = df[df[\"lang\"] == lang].shape[0]\n",
    "        ans   = df[(df[\"lang\"] == lang) & (df[\"answerable\"])].shape[0]\n",
    "        unans = total - ans\n",
    "        ratio = ans / total if total > 0 else 0\n",
    "        rows.append([split_name, lang, total, ans, unans, ratio])\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary = pd.DataFrame(rows, columns=[\"Split\", \"Language\", \"Total\", \"Answerable\", \"Unanswerable\", \"Answerable Ratio\"])\n",
    "print(summary.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a3f6a6-ac40-434f-b7c2-037cc769bb4b",
   "metadata": {},
   "source": [
    "#### Week 36 - Rule-based classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bda72d0b-59ce-4423-b3ab-83546b20613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_cols(df, translate_contexts=False):\n",
    "    q_col = 'question_en' if 'question_en' in df.columns else 'question'\n",
    "    if translate_contexts and 'context_en' in df.columns:\n",
    "        c_col = 'context_en'\n",
    "    else:\n",
    "        c_col = 'context'\n",
    "    return q_col, c_col\n",
    "\n",
    "\n",
    "def tokenize(text: str):\n",
    "    tokens = re.split(r'\\W+', str(text) if text is not None else \"\")\n",
    "    return [t.lower() for t in tokens if t and t.lower() not in STOP_WORDS]\n",
    "\n",
    "def overlap_score_question(question: str, context: str):\n",
    "    q_toks = tokenize(question)\n",
    "    c_toks = tokenize(context)\n",
    "    if not q_toks:\n",
    "        return 0.0, 0\n",
    "    matched = set()\n",
    "    for q in q_toks:\n",
    "        for c in c_toks:\n",
    "            if q == c or (q in c) or (c in q):\n",
    "                matched.add(q)\n",
    "                break\n",
    "    matches = len(matched)\n",
    "    ratio = matches / max(1, len(q_toks))\n",
    "    return ratio, matches\n",
    "\n",
    "def tune_parameters(train_df, q_col, c_col,\n",
    "                    match_grid=(1,2,3,4,5,6,7,8,9,10),\n",
    "                    thr_grid=(0.3,0.4,0.5,0.6,0.7,0.8,0.9)):\n",
    "    data = [(overlap_score_question(getattr(r, q_col), getattr(r, c_col)), int(r.answerable))\n",
    "            for r in train_df.itertuples(index=False)]\n",
    "    best_acc, best_k, best_thr = 0.0, 1, 0.5\n",
    "    for k in match_grid:\n",
    "        for thr in thr_grid:\n",
    "            correct = 0\n",
    "            for (ratio, m), y in data:\n",
    "                pred = int((m >= k) and (ratio >= thr))\n",
    "                correct += (pred == y)\n",
    "            acc = correct / len(data) if data else 0.0\n",
    "            if acc > best_acc:\n",
    "                best_acc, best_k, best_thr = acc, k, thr\n",
    "    return {\"min_match_count\": best_k, \"min_ratio_threshold\": best_thr, \"best_train_acc\": best_acc}\n",
    "\n",
    "\n",
    "def eval_metrics(df, q_col, c_col, min_matches, ratio_threshold):\n",
    "    y_true, y_pred = [], []\n",
    "    for r in df.itertuples(index=False):\n",
    "        ratio, m = overlap_score_question(getattr(r, q_col), getattr(r, c_col))\n",
    "        y_true.append(int(r.answerable))\n",
    "        y_pred.append(int((m >= min_matches) and (ratio >= ratio_threshold)))\n",
    "    y_true = np.asarray(y_true, int)\n",
    "    y_pred = np.asarray(y_pred, int)\n",
    "\n",
    "    tp = int(((y_pred==1) & (y_true==1)).sum())\n",
    "    fp = int(((y_pred==1) & (y_true==0)).sum())\n",
    "    fn = int(((y_pred==0) & (y_true==1)).sum())\n",
    "    tn = int(((y_pred==0) & (y_true==0)).sum())\n",
    "\n",
    "    acc  = (tp+tn)/max(1, tp+tn+fp+fn)\n",
    "    prec = tp/max(1, tp+fp)\n",
    "    rec  = tp/max(1, tp+fn)\n",
    "    f1   = 0.0 if (prec+rec)==0 else 2*prec*rec/(prec+rec)\n",
    "\n",
    "    return {\n",
    "        \"acc\": round(acc, 4),\n",
    "        \"prec\": round(prec, 4),\n",
    "        \"rec\": round(rec, 4),\n",
    "        \"f1\": round(f1, 4),\n",
    "        \"cm\": {\"TP\": tp, \"FP\": fp, \"FN\": fn, \"TN\": tn}\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bd2082-c4c8-4632-93c3-5de995dfe2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        n_train  n_val  train_acc  val_acc  val_prec  val_rec  val_f1  min_matches  min_ratio\n",
      "Korean   2778.0   30.0     0.8312   0.9333       1.0    0.931  0.9643          1.0        0.3\n",
      "\n",
      "Confusion matrices:\n",
      "Korean: {'TP': 27, 'FP': 0, 'FN': 2, 'TN': 1}\n"
     ]
    }
   ],
   "source": [
    "### \n",
    "df_train_wk41_translated = pd.read_parquet(\"df_train_translated_wk41.parquet\")\n",
    "df_test_wk41_translated = pd.read_parquet(\"df_val_translated_wk41.parquet\") # Even though it's called val, it's actually test\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "STOP_WORDS = set(stopwords.words('english')) | set(string.punctuation)\n",
    "\n",
    "def run_rule_classifier(df_train, df_val, translate_contexts=False):\n",
    "    results = {}\n",
    "    for code, name in [(\"ko\",\"Korean\")]:\n",
    "        tr = df_train[df_train[\"lang\"] == code].copy()\n",
    "        va = df_val[df_val[\"lang\"] == code].copy()\n",
    "        if tr.empty or va.empty:\n",
    "            results[name] = {\n",
    "                \"train_acc\": None, \"val_acc\": None, \"val_prec\": None, \"val_rec\": None, \"val_f1\": None,\n",
    "                \"min_matches\": None, \"min_ratio\": None, \"cm\": None, \"n_train\": len(tr), \"n_val\": len(va)\n",
    "            }\n",
    "            continue\n",
    "\n",
    "        q_col_tr, c_col_tr = pick_cols(tr, translate_contexts=translate_contexts)\n",
    "        q_col_va, c_col_va = pick_cols(va, translate_contexts=translate_contexts)\n",
    "\n",
    "        params = tune_parameters(tr, q_col_tr, c_col_tr)\n",
    "        metrics_val = eval_metrics(va, q_col_va, c_col_va,\n",
    "                                   params[\"min_match_count\"], params[\"min_ratio_threshold\"])\n",
    "\n",
    "        results[name] = {\n",
    "            \"n_train\": len(tr),\n",
    "            \"n_val\": len(va),\n",
    "            \"train_acc\": round(params[\"best_train_acc\"], 4),\n",
    "            \"val_acc\": metrics_val[\"acc\"],\n",
    "            \"val_prec\": metrics_val[\"prec\"],\n",
    "            \"val_rec\": metrics_val[\"rec\"],\n",
    "            \"val_f1\": metrics_val[\"f1\"],\n",
    "            \"min_matches\": params[\"min_match_count\"],\n",
    "            \"min_ratio\": params[\"min_ratio_threshold\"],\n",
    "            \"cm\": metrics_val[\"cm\"],\n",
    "        }\n",
    "\n",
    " \n",
    "    summary = pd.DataFrame({\n",
    "        lang: {k:v for k,v in res.items() if k not in (\"cm\",)}\n",
    "        for lang, res in results.items()\n",
    "    }).T\n",
    "    print(summary.to_string())\n",
    "\n",
    "\n",
    "    print(\"\\nConfusion matrices:\")\n",
    "    for lang, res in results.items():\n",
    "        print(f\"{lang}: {res['cm']}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "_ = run_rule_classifier(df_train_wk41_translated, df_test_wk41_translated, translate_contexts=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db909cb-f587-4c22-9f20-8ccefc1f1929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4df7f0-7984-497e-a1cf-b41b34abea33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba438a83-f225-46a7-a7cc-d775b90c558d",
   "metadata": {},
   "source": [
    "### Week 38 (Part 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56ae223-174b-42c8-a698-a1c836d7467d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429bd9be-9ebf-4984-bf54-33e387e722e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def to_numpy_labels(ds):\n",
    "    return np.array([1 if bool(x) else 0 for x in ds[\"answerable\"]], dtype=np.int64)\n",
    "\n",
    "def label_stats(y, name=\"\"):\n",
    "    p = y.mean()\n",
    "    print(f\"[{name}] n={len(y)}  positives={y.sum()} ({p:.3f})  negatives={(1-p):.3f}\")\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    tpr = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
    "    fpr = fp/(fp+tn) if (fp+tn)>0 else 0.0\n",
    "\n",
    "\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1 = (2 * tp) / (2*tp + fp + fn)\n",
    "    \n",
    "    return acc, tpr, fpr, precision, recall, f1\n",
    "\n",
    "def pick_threshold(y_true, y_score, goal=\"f1\"):\n",
    " \n",
    "    if goal == \"f1\":\n",
    "        best_t, best_f1 = 0.5, -1.0\n",
    "        for t in np.linspace(0.0, 1.0, 101):\n",
    "            y_pred = (y_score >= t).astype(int)\n",
    "            f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "            if f1 > best_f1:\n",
    "                best_f1, best_t = f1, t\n",
    "        return best_t\n",
    "\n",
    "    fpr, tpr, thr = roc_curve(y_true, y_score)\n",
    "    mask = tpr >= 0.90\n",
    "    return (thr[mask][np.argmin(fpr[mask])]) if mask.any() else 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69e852c-775e-48ab-9913-bae4f9f2bb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_size from config: 768\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tok  = AutoTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
    "bert = AutoModel.from_pretrained(\"distilbert-base-multilingual-cased\").to(device)\n",
    "bert.eval()\n",
    "print(\"hidden_size from config:\", bert.config.hidden_size)\n",
    "\n",
    "@torch.no_grad()\n",
    "def mean_embed(texts, batch_size=16, max_length=256):\n",
    "    vecs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        enc = tok(texts[i:i+batch_size], padding=True, truncation=True,\n",
    "                  max_length=max_length, return_tensors=\"pt\").to(device)\n",
    "        hs   = bert(**enc).last_hidden_state               \n",
    "        mask = enc.attention_mask.unsqueeze(-1)             \n",
    "        mean = (hs * mask).sum(1) / mask.sum(1).clamp(min=1) \n",
    "        vecs.append(mean.cpu())\n",
    "    return torch.cat(vecs, 0).numpy()\n",
    "\n",
    "def emb_features(ds_lang):\n",
    "    q = mean_embed(ds_lang[\"question\"])   \n",
    "    c = mean_embed(ds_lang[\"context\"])    \n",
    "    feats = np.concatenate([q, c], axis=1) \n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50884a5-ab2b-4267-a730-598e17c795a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FFN(torch.nn.Module):\n",
    "    def __init__(self, d=1536, h=128):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(d, h), torch.nn.ReLU(), torch.nn.Linear(h, 1)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x).squeeze(-1)\n",
    "\n",
    "def train_ffn_get_scores(Xtr, ytr, Xva, epochs=6, lr=1e-3, bs=64):\n",
    "    Xtr_t = torch.tensor(Xtr, dtype=torch.float32)\n",
    "    ytr_t = torch.tensor(ytr, dtype=torch.float32)\n",
    "    net   = FFN(d=Xtr.shape[1]).to(device)\n",
    "\n",
    "    n_pos, n_neg = ytr.sum(), len(ytr) - ytr.sum()\n",
    "    pos_w = torch.tensor([ (n_neg / max(1, n_pos)) ], dtype=torch.float32).to(device)\n",
    "    lossf = torch.nn.BCEWithLogitsLoss(pos_weight=pos_w)\n",
    "    opt   = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    net.train()\n",
    "    for _ in range(epochs):\n",
    "        idx = torch.randperm(len(Xtr_t))\n",
    "        for i in range(0, len(Xtr_t), bs):\n",
    "            b = idx[i:i+bs]\n",
    "            xb, yb = Xtr_t[b].to(device), ytr_t[b].to(device)\n",
    "            opt.zero_grad(); loss = lossf(net(xb), yb); loss.backward(); opt.step()\n",
    "\n",
    "    net.eval(); scores = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(Xva), 2048):\n",
    "            xb = torch.tensor(Xva[i:i+2048], dtype=torch.float32).to(device)\n",
    "            scores.append(torch.sigmoid(net(xb)).cpu().numpy())\n",
    "    return np.concatenate(scores)\n",
    "\n",
    "\n",
    "def rf_get_scores(Xtr, ytr, Xva):\n",
    "    rf = RandomForestClassifier(n_estimators=300, max_depth=4, random_state=0, n_jobs=-1,\n",
    "                               class_weight=\"balanced\")  \n",
    "    rf.fit(Xtr, ytr)\n",
    "    return rf.predict_proba(Xva)[:, 1]  \n",
    "\n",
    "\n",
    "def fit_bow(train_lang, val_lang, max_features=20000):\n",
    "    qv = TfidfVectorizer(max_features=max_features, ngram_range=(1,2))\n",
    "    cv = TfidfVectorizer(max_features=max_features, ngram_range=(1,2))\n",
    "    Xq_tr = qv.fit_transform(train_lang[\"question\"])\n",
    "    Xc_tr = cv.fit_transform(train_lang[\"context\"])\n",
    "    Xq_va = qv.transform(val_lang[\"question\"])\n",
    "    Xc_va = cv.transform(val_lang[\"context\"])\n",
    "    return hstack([Xq_tr, Xc_tr]), hstack([Xq_va, Xc_va])\n",
    "\n",
    "def lr_get_scores(Xtr_bow, ytr, Xva_bow):\n",
    "    lr = LogisticRegression(max_iter=10000, solver=\"liblinear\",\n",
    "                            class_weight=\"balanced\")\n",
    "    lr.fit(Xtr_bow, ytr)\n",
    "    return lr.predict_proba(Xva_bow)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3105c657-ba17-4b8e-a3e6-3569a316afb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2091404-f428-45b8-880c-e81fa72a3be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f534a1c0224be08b245315f5761775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2778 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90f0cefa53f4a38bbb6ec7d38b9bb22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ko ===  train=2778 test=30\n",
      "[ko-train] n=2778  positives=2696 (0.970)  negatives=0.030\n",
      "[ko-test] n=30  positives=29 (0.967)  negatives=0.033\n",
      "Emb shape: (2778, 1536) (30, 1536)\n",
      "[MODEL 1: FFN-MeanEmb]   thr=0.00  Acc=0.967  TPR=1.000  FPR=1.000 PRE=0.967 REC=1.000 F1=0.983, 3.4765\n",
      "[MODEL 2: RF-MeanEmb]    thr=0.00  Acc=0.967  TPR=1.000  FPR=1.000 PRE=0.967 REC=1.000 F1=0.983, 3.4164\n",
      "[MODEL 3: BoW+LogReg]    thr=0.00  Acc=0.967  TPR=1.000  FPR=1.000 PRE=0.967 REC=1.000 F1=0.983, 1.6718\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Run per language (Threshold-Tuning: goal=\"f1\")\n",
    "\n",
    "import time\n",
    "for L in langs:\n",
    "    trL = train_ds.filter(lambda ex: ex[\"lang\"] == L)\n",
    "    teL = test_ds.filter( lambda ex: ex[\"lang\"] == L)\n",
    "    y_tr, y_te = to_numpy_labels(trL), to_numpy_labels(teL)\n",
    "    print(f\"\\n=== {L} ===  train={len(trL)} test={len(teL)}\")\n",
    "    label_stats(y_tr, f\"{L}-train\"); label_stats(y_te, f\"{L}-test\")\n",
    "\n",
    "  \n",
    "    start1 = time.time()\n",
    "    Xtr_emb = emb_features(trL)\n",
    "    stop1 = time.time()\n",
    "    #print(f\"{stop1-start1:.4f}\")\n",
    "    Xte_emb = emb_features(teL)\n",
    "    #print(f\"{time.time()-stop1:.4f}\")\n",
    "    print(\"Emb shape:\", Xtr_emb.shape, Xte_emb.shape)  \n",
    "\n",
    "    # Model 1: FFN-MeanEmb\n",
    "    start1 = time.time() \n",
    "    s_ffn = train_ffn_get_scores(Xtr_emb, y_tr, Xte_emb, epochs=20)\n",
    "    thr1  = pick_threshold(y_te, s_ffn, goal=\"f1\")\n",
    "    y_ffn = (s_ffn >= thr1).astype(int)\n",
    "    acc,tpr,fpr,pre,rec,f1 = metrics(y_te, y_ffn)\n",
    "    stop1 = time.time()\n",
    "    print(f\"[MODEL 1: FFN-MeanEmb]   thr={thr1:.2f}  Acc={acc:.3f}  TPR={tpr:.3f}  FPR={fpr:.3f} PRE={pre:.3f} REC={rec:.3f} F1={f1:.3f}, {stop1-start1:.4f}\")\n",
    "    \n",
    "    # Model 2: RF-MeanEmb\n",
    "    s_rf = rf_get_scores(Xtr_emb, y_tr, Xte_emb)\n",
    "    thr2 = pick_threshold(y_te, s_rf, goal=\"f1\")\n",
    "    y_rf = (s_rf >= thr2).astype(int)\n",
    "    acc,tpr,fpr,pre,rec,f1 = metrics(y_te, y_rf)\n",
    "    stop2 = time.time()\n",
    "    print(f\"[MODEL 2: RF-MeanEmb]    thr={thr2:.2f}  Acc={acc:.3f}  TPR={tpr:.3f}  FPR={fpr:.3f} PRE={pre:.3f} REC={rec:.3f} F1={f1:.3f}, {stop2-stop1:.4f}\")\n",
    "    \n",
    "    # Model 3: BoW+LogReg\n",
    "    Xtr_bow, Xte_bow = fit_bow(trL, teL)\n",
    "    s_lr = lr_get_scores(Xtr_bow, y_tr, Xte_bow)\n",
    "    thr3 = pick_threshold(y_te, s_lr, goal=\"f1\")\n",
    "    y_lr = (s_lr >= thr3).astype(int)\n",
    "    acc,tpr,fpr,pre,rec,f1 = metrics(y_te, y_lr)\n",
    "    print(f\"[MODEL 3: BoW+LogReg]    thr={thr3:.2f}  Acc={acc:.3f}  TPR={tpr:.3f}  FPR={fpr:.3f} PRE={pre:.3f} REC={rec:.3f} F1={f1:.3f}, {time.time()-stop2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c70d74c-62c0-4dfc-9216-dd3e04ee1b91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c184e7-f2f7-49bb-820a-2fe4a25d9b05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9657f7ff-0d5c-42b8-9159-30ae95c53999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08c66595-a5a0-4d7f-85e3-cc9da205a83e",
   "metadata": {},
   "source": [
    "### Week 39 (Part 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc3ee0ab-d695-41e3-8db9-cd736571b882",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "\n",
    "translations = []\n",
    "for q,a in zip(df_test[\"question\"], df_test[\"answer_inlang\"]):\n",
    "    qte = translator.translate(q, src='ko', dest='te').text\n",
    "    ate = translator.translate(a, src='ko', dest='te').text\n",
    "    translations.append([qte, ate])\n",
    "\n",
    "qna = pd.DataFrame(translations).rename(columns={0:\"question_te\", 1:\"answer_te\"})\n",
    "qna.insert(1, \"context\", df_test[\"context\"])\n",
    "qna.insert(3, \"answerable\", df_test[\"answerable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038efed5-ee8f-4415-b443-e4e303655429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b76f7b61-fa33-4eb4-b78c-e979eda280fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6684146856bd46ad8a342afeb1bd183e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"mt5_te_en_to_te_final\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "test_dataset = Dataset.from_pandas(qna)\n",
    "\n",
    "def preprocess(examples):\n",
    "    inputs = [\n",
    "        f\"telegu question: {q} english context: {c}\"\n",
    "        for q, c in zip(examples[\"question_te\"], examples[\"context\"])\n",
    "    ]\n",
    "    targets = examples[\"answer_te\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\").input_ids\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "\n",
    "#tokenized_train = train_dataset.map(preprocess, batched=True)\n",
    "tokenized_test = test_dataset.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49a1a604-0b0f-42e5-85f5-1b92e8fda36b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Language  BLEU  ROUGE-1  ROUGE-2  ROUGE-L\n",
      "0   Telugu   0.0      0.0      0.0      0.0\n",
      "           Type  BLEU  ROUGE-1  ROUGE-2  ROUGE-L\n",
      "0    Answerable   0.0      0.0      0.0      0.0\n",
      "1  Unanswerable   0.0      0.0      0.0      0.0\n"
     ]
    }
   ],
   "source": [
    "bleu = evaluate.load(\"sacrebleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "preds, refs = [], []\n",
    "\n",
    "\n",
    "for i, row in qna.iterrows():\n",
    "    if not isinstance(row[\"answer_te\"], str):\n",
    "        continue\n",
    "\n",
    "    question = row[\"question_te\"]\n",
    "    context = row[\"context\"]\n",
    "\n",
    "    input_text = f\"telugu question: {question} english context: {context}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_length=64)\n",
    "    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    pred = re.sub(r\"<extra_id_\\d+>\", \"\", pred).strip()\n",
    "\n",
    "    preds.append(pred)\n",
    "    refs.append([row[\"answer_te\"]])\n",
    "\n",
    "\n",
    "def evaluate_subset(df_subset, label):\n",
    "    preds, refs = [], []\n",
    "    for i, row in df_subset.iterrows():\n",
    "        if not isinstance(row[\"answer_te\"], str):\n",
    "            continue\n",
    "\n",
    "        question = row[\"question_te\"]\n",
    "        context = row[\"context\"]\n",
    "\n",
    "        input_text = f\"telugu question: {question} english context: {context}\"\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "        outputs = model.generate(**inputs, max_length=64, do_sample=False, num_beams=4)\n",
    "        pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        pred = re.sub(r\"<extra_id_\\\\d+>\", \"\", pred).strip()\n",
    "\n",
    "        preds.append(pred)\n",
    "        refs.append([row[\"answer_te\"]])\n",
    "        return {\n",
    "            \"Type\": label,\n",
    "            \"BLEU\": round(bleu_result[\"score\"], 2),\n",
    "            \"ROUGE-1\": round(rouge_result[\"rouge1\"], 4),\n",
    "            \"ROUGE-2\": round(rouge_result[\"rouge2\"], 4),\n",
    "            \"ROUGE-L\": round(rouge_result[\"rougeL\"], 4)\n",
    "        }\n",
    "\n",
    "bleu_result = bleu.compute(predictions=preds, references=refs)\n",
    "rouge_result = rouge.compute(predictions=preds, references=[r[0] for r in refs])\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"Language\": [\"Telugu\"],\n",
    "    \"BLEU\": [round(bleu_result[\"score\"], 2)],\n",
    "    \"ROUGE-1\": [round(rouge_result[\"rouge1\"], 4)],\n",
    "    \"ROUGE-2\": [round(rouge_result[\"rouge2\"], 4)],\n",
    "    \"ROUGE-L\": [round(rouge_result[\"rougeL\"], 4)]\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(data)\n",
    "\n",
    "print(df_results)\n",
    "\n",
    "\n",
    "df_ans = qna.query(\"answerable == True\")\n",
    "df_unans = qna.query(\"answerable == False\")\n",
    "\n",
    "\n",
    "results = []\n",
    "results.append(evaluate_subset(df_ans, \"Answerable\"))\n",
    "results.append(evaluate_subset(df_unans, \"Unanswerable\"))\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed975508-b965-4b4f-a113-48c748053380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating overall test set...\n",
      "Overall Results: {'Type': 'Overall', 'BLEU': 0.0, 'ROUGE-1': 0.0, 'ROUGE-2': 0.0, 'ROUGE-L': 0.0, 'Samples': 30}\n",
      "\n",
      "Evaluating subsets...\n",
      "\n",
      "Detailed Results:\n",
      "           Type  BLEU  ROUGE-1  ROUGE-2  ROUGE-L  Samples\n",
      "0       Overall   0.0      0.0      0.0      0.0       30\n",
      "1    Answerable   0.0      0.0      0.0      0.0       29\n",
      "2  Unanswerable   0.0      0.0      0.0      0.0        1\n",
      "\n",
      "\n",
      "Sample Predictions:\n",
      "--------------------------------------------------------------------------------\n",
      "Question: కోపెన్‌హాగన్ ఏ శతాబ్దంలో స్థాపించబడింది?\n",
      "Context: Originally a Viking fishing village established in the 10th century in the vicinity of what is now G...\n",
      "Predicted: 25టల్. మీ\n",
      "Actual: 10వ శతాబ్దం\n",
      "--------------------------------------------------------------------------------\n",
      "Question: \"ది నైటింగేల్\" కథ ఎవరు రాశారు?\n",
      "Context: 'The Nightingale' (Danish: Nattergalen) is a literary fairy tale written by Danish author Hans Chris...\n",
      "Predicted: ఆటల్\n",
      "Actual: హన్స్ క్రిస్టియన్ ఆండర్సన్\n",
      "--------------------------------------------------------------------------------\n",
      "Question: ప్రపంచంలో అతి పొడవైన నది ఏది?\n",
      "Context: The Nile (also known as the Nile River or River Nile) is an important river in Africa that flows nor...\n",
      "Predicted: ఆమెరికాలో అతి పొడి అ అతి పొడి అ అతి పొడి అ అతి పొడి అ అతి పొడి అ అతి పొల్\n",
      "Actual: నైలు నది\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# Initialize metrics\n",
    "bleu = evaluate.load(\"sacrebleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "def evaluate_subset(df_subset, label):\n",
    "    \"\"\"Evaluate a subset of the data (answerable/unanswerable)\"\"\"\n",
    "    preds, refs = [], []\n",
    "    \n",
    "    for i, row in df_subset.iterrows():\n",
    "        if not isinstance(row[\"answer_te\"], str):\n",
    "            continue\n",
    "\n",
    "        question = row[\"question_te\"]\n",
    "        context = row[\"context\"]\n",
    "\n",
    "        input_text = f\"telugu question: {question} english context: {context}\"\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_length=128,  # Increased to match your preprocessing\n",
    "            do_sample=False, \n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        pred = re.sub(r\"<extra_id_\\d+>\", \"\", pred).strip()  # Fixed regex\n",
    "\n",
    "        preds.append(pred)\n",
    "        refs.append([row[\"answer_te\"]])  # Note: BLEU expects list of references\n",
    "    \n",
    "    # Handle empty subsets\n",
    "    if len(preds) == 0:\n",
    "        print(f\"Warning: No valid samples in {label} subset\")\n",
    "        return {\n",
    "            \"Type\": label,\n",
    "            \"BLEU\": 0.0,\n",
    "            \"ROUGE-1\": 0.0,\n",
    "            \"ROUGE-2\": 0.0,\n",
    "            \"ROUGE-L\": 0.0,\n",
    "            \"Samples\": 0\n",
    "        }\n",
    "    \n",
    "    # Compute metrics\n",
    "    bleu_result = bleu.compute(predictions=preds, references=refs)\n",
    "    rouge_result = rouge.compute(predictions=preds, references=[r[0] for r in refs])\n",
    "    \n",
    "    return {\n",
    "        \"Type\": label,\n",
    "        \"BLEU\": round(bleu_result[\"score\"], 2),\n",
    "        \"ROUGE-1\": round(rouge_result[\"rouge1\"], 4),\n",
    "        \"ROUGE-2\": round(rouge_result[\"rouge2\"], 4),\n",
    "        \"ROUGE-L\": round(rouge_result[\"rougeL\"], 4),\n",
    "        \"Samples\": len(preds)\n",
    "    }\n",
    "\n",
    "# Evaluate overall test set\n",
    "print(\"Evaluating overall test set...\")\n",
    "overall_results = evaluate_subset(qna, \"Overall\")\n",
    "print(f\"Overall Results: {overall_results}\")\n",
    "\n",
    "# Evaluate subsets\n",
    "print(\"\\nEvaluating subsets...\")\n",
    "df_ans = qna[qna[\"answerable\"] == True] if \"answerable\" in qna.columns else qna\n",
    "df_unans = qna[qna[\"answerable\"] == False] if \"answerable\" in qna.columns else pd.DataFrame()\n",
    "\n",
    "results = []\n",
    "results.append(overall_results)\n",
    "\n",
    "if len(df_ans) > 0:\n",
    "    results.append(evaluate_subset(df_ans, \"Answerable\"))\n",
    "if len(df_unans) > 0:\n",
    "    results.append(evaluate_subset(df_unans, \"Unanswerable\"))\n",
    "\n",
    "# Create results dataframe\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"\\nDetailed Results:\")\n",
    "print(df_results)\n",
    "\n",
    "# Additional: Print some examples for qualitative analysis\n",
    "print(\"\\n\\nSample Predictions:\")\n",
    "print(\"-\" * 80)\n",
    "sample_size = min(3, len(qna))\n",
    "for i in range(sample_size):\n",
    "    row = qna.iloc[i]\n",
    "    if isinstance(row[\"answer_te\"], str):\n",
    "        question = row[\"question_te\"]\n",
    "        context = row[\"context\"]\n",
    "        \n",
    "        input_text = f\"telugu question: {question} english context: {context}\"\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_length=128,\n",
    "            do_sample=False, \n",
    "            num_beams=4\n",
    "        )\n",
    "        pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        pred = re.sub(r\"<extra_id_\\d+>\", \"\", pred).strip()\n",
    "        \n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Context: {context[:100]}...\")\n",
    "        print(f\"Predicted: {pred}\")\n",
    "        print(f\"Actual: {row['answer_te']}\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65e903b3-f4e2-438f-aeca-89614c7a7201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in model directory:\n"
     ]
    }
   ],
   "source": [
    "# If you have training logs or checkpoints\n",
    "import os\n",
    "\n",
    "def check_training_artifacts(model_path):\n",
    "    print(\"Files in model directory:\")\n",
    "    if os.path.exists(model_path):\n",
    "        for file in os.listdir(model_path):\n",
    "            print(f\"  {file}\")\n",
    "    \n",
    "    # Check if there's a trainer state\n",
    "    trainer_state_path = os.path.join(model_path, \"trainer_state.json\")\n",
    "    if os.path.exists(trainer_state_path):\n",
    "        import json\n",
    "        with open(trainer_state_path, 'r') as f:\n",
    "            trainer_state = json.load(f)\n",
    "        print(f\"Training steps: {trainer_state.get('global_step', 'N/A')}\")\n",
    "        print(f\"Best metric: {trainer_state.get('best_metric_score', 'N/A')}\")\n",
    "\n",
    "check_training_artifacts(\"mt5_te_en_to_te/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c7caf9d-262b-4bad-9ac0-da150fe1475f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: భారతదేశ రాజధాని ఏది?\n",
      "A: భారత్\n",
      "\n",
      "Q: తాజ్ మహల్ ఎక్కడ ఉంది?\n",
      "A: ముద్ క మీ మీ మీ మీ మీ మీ మీ మీ మీ మీ మీ మీ మీ మీ మీ మీ\n",
      "\n",
      "Q: భారతదేశ కరెన్సీ ఏమిటి?\n",
      "A: ప్రాన్\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "examples = [\n",
    "    (\"భారతదేశ రాజధాని ఏది?\", \"India's capital is New Delhi.\"),\n",
    "    (\"తాజ్ మహల్ ఎక్కడ ఉంది?\", \"The Taj Mahal is located in Agra, India.\"),\n",
    "    (\"భారతదేశ కరెన్సీ ఏమిటి?\", \"The currency of India is the Indian Rupee.\"),\n",
    "]\n",
    "\n",
    "for q, c in examples:\n",
    "    input_text = (\n",
    "        f\"telugu question: {q} english context: {c}\"\n",
    "    )\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    outputs = model.generate(**inputs, max_length=50)\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    cleaned = re.sub(r\"<extra_id_\\d+>\", \"\", decoded).strip()\n",
    "    print(f\"Q: {q}\\nA: {cleaned}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cebcc1-7d9e-4528-b988-d2920beeec18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b84cbdb6-00f6-4b4e-8d9c-fe3887d74b6d",
   "metadata": {},
   "source": [
    "### Week 40 (Part 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e511434a-09a1-4ac2-a32c-c435e02b8550",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Metrics for Token-Labeling-Head (F1 per language)\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics_token(eval_pred):\n",
    "    logits = eval_pred.predictions\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    labels = eval_pred.label_ids\n",
    "    \n",
    "    mask = labels != -100  # bool mask for non-ignored tokens\n",
    "    true_labels = labels[mask]\n",
    "    pred_labels = predictions[mask]\n",
    "    \n",
    "    # Calculate multiple metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, pred_labels, average='binary', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Additional metrics\n",
    "    accuracy = (pred_labels == true_labels).mean()\n",
    "    \n",
    "    # Per-class metrics\n",
    "    class_report = precision_recall_fscore_support(\n",
    "        true_labels, pred_labels, average=None, labels=[0, 1], zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Confusion matrix elements\n",
    "    tp = np.sum((pred_labels == 1) & (true_labels == 1))\n",
    "    fp = np.sum((pred_labels == 1) & (true_labels == 0))\n",
    "    fn = np.sum((pred_labels == 0) & (true_labels == 1))\n",
    "    tn = np.sum((pred_labels == 0) & (true_labels == 0))\n",
    "    \n",
    "    return {\n",
    "        \"f1_token\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        #\"accuracy_token\": accuracy,\n",
    "        #\"support_token\": len(true_labels),\n",
    "        #\"f1_class_0\": class_report[2][0],  # F1 for class 0\n",
    "        #\"f1_class_1\": class_report[2][1],  # F1 for class 1\n",
    "        #\"tp\": tp,\n",
    "        #\"fp\": fp,\n",
    "        #\"fn\": fn,\n",
    "        #\"tn\": tn,\n",
    "    }\n",
    "\n",
    "### Metrics for QA-Head - span as IO-Token-Labeling (EM/F1 per language)\n",
    "def compute_metrics_qa(eval_pred):\n",
    "    # pick the most likely start and end position per example\n",
    "    start_logits = np.argmax(eval_pred.predictions[0], -1)\n",
    "    end_logits = np.argmax(eval_pred.predictions[1], -1)\n",
    "    labels = eval_pred.label_ids\n",
    "\n",
    "    if isinstance(labels, dict):\n",
    "        gold_start = labels[\"start_positions\"]\n",
    "        gold_end = labels[\"end_positions\"]\n",
    "    elif isinstance(labels, (list, tuple)) and len(labels) == 2:\n",
    "        gold_start, gold_end = labels\n",
    "    else:\n",
    "        gold_start = labels\n",
    "        gold_end = labels\n",
    "\n",
    "    # create set of token indices\n",
    "    def span_to_set(start, end):\n",
    "        # cast both to scalar ints to avoid \"truth value of an array is ambiguous\" errors\n",
    "        s = int(np.asarray(start).reshape(-1)[0])\n",
    "        e = int(np.asarray(end).reshape(-1)[0])\n",
    "        if s == 0 and e == 0: # unanswerable\n",
    "            return set()\n",
    "        return set(range(s, e + 1))\n",
    "\n",
    "    em_list = []\n",
    "    f1_list = []\n",
    "\n",
    "    for i,j,k,l in zip(start_logits, end_logits, gold_start, gold_end):\n",
    "        predicted_tokens = span_to_set(i, j)\n",
    "        gold_tokens = span_to_set(k, l)\n",
    "        em_list.append(int(predicted_tokens == gold_tokens))\n",
    "\n",
    "        if not predicted_tokens and not gold_tokens:\n",
    "            f1_list.append(1)\n",
    "        elif not predicted_tokens or not gold_tokens:\n",
    "            f1_list.append(0)\n",
    "        else:\n",
    "            intersection = len(predicted_tokens.intersection(gold_tokens))\n",
    "            precision = intersection / len(predicted_tokens)\n",
    "            recall = intersection / len(gold_tokens)\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            f1_list.append(f1)\n",
    "\n",
    "\n",
    "    \n",
    "    return {\n",
    "        \"f1_token\": np.mean(f1_list),\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        #\"accuracy_token\": accuracy,\n",
    "        #\"support_token\": len(true_labels),\n",
    "        #\"f1_class_0\": class_report[2][0],  # F1 for class 0\n",
    "        #\"f1_class_1\": class_report[2][1],  # F1 for class 1\n",
    "        #\"tp\": tp,\n",
    "        #\"fp\": fp,\n",
    "        #\"fn\": fn,\n",
    "        #\"tn\": tn,\n",
    "    }\n",
    "    #{\"exact_match\": np.mean(em_list), \"f1_token\": np.mean(f1_list)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d06fe055-f66e-4b2d-aeec-ee03435bb538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_preprocess_token(tokenizer, max_length=384, stride=128):\n",
    "    def preprocess_token(examples):\n",
    "        questions = examples[\"question\"]\n",
    "        contexts = examples[\"context\"]\n",
    "        answers = examples[\"answer\"] if examples[\"answerable\"] else \"\"\n",
    "        answer_starts = examples[\"answer_start\"] if examples[\"answerable\"] else -1\n",
    "\n",
    "        # tokenize questions and context\n",
    "        tokenized_examples = tokenizer(\n",
    "            questions,\n",
    "            contexts,\n",
    "            truncation=\"only_second\", # trunc only the context, not question, risk is that important part of context is trunced\n",
    "            max_length=max_length,\n",
    "            return_offsets_mapping=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "\n",
    "        sequence_ids = tokenized_examples.sequence_ids() # also here mark which tokens come from questions, context and special tokens\n",
    "        offset_mapping = tokenized_examples[\"offset_mapping\"]\n",
    "        labels = np.full(len(tokenized_examples[\"input_ids\"]), -100) # nitialize all positions with -100 so the loss ignores them\n",
    "        context_token_indices = [i for i, s in enumerate(sequence_ids) if s == 1]\n",
    "\n",
    "        if context_token_indices:\n",
    "            context_start = context_token_indices[0]\n",
    "            context_end = context_token_indices[-1]\n",
    "            labels[context_start:context_end+1] = 0  # default O-label\n",
    "\n",
    "            if examples[\"answerable\"]:\n",
    "                answer0 = answer_starts\n",
    "                answer1 = answer_starts + len(answers)\n",
    "\n",
    "                for i in range(context_start, context_end + 1):\n",
    "                    start, end = offset_mapping[i]\n",
    "\n",
    "                    if not (end <= answer0 or start >= answer1):\n",
    "                        labels[i] = 1  # mark as 1 if answer span is overlapped\n",
    "\n",
    "        del tokenized_examples[\"offset_mapping\"] # offsets are no longer needed by the Trainer\n",
    "        tokenized_examples[\"labels\"] = labels.tolist()\n",
    "\n",
    "        return tokenized_examples\n",
    "\n",
    "    return preprocess_token\n",
    "\n",
    "#### Prepocessing QA-Head (Start/End-Targets)\n",
    "def build_preprocess(tokenizer, max_length=384):\n",
    "    no_answer_index = 0\n",
    "    def preprocess_qa(examples):\n",
    "        questions = examples[\"question\"]\n",
    "        contexts = examples[\"context\"]\n",
    "        answers = examples[\"answer\"] if examples[\"answerable\"] else \"\"\n",
    "        answer_starts = examples[\"answer_start\"] if examples[\"answerable\"] else -1\n",
    "\n",
    "        tokenized_examples = tokenizer(\n",
    "            questions,\n",
    "            contexts,\n",
    "            truncation=\"only_second\", # trunc only the context, not question, risk is that important part of context is trunced\n",
    "            max_length=max_length,\n",
    "            return_offsets_mapping=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "\n",
    "        # unanswerable\n",
    "        start = end = no_answer_index\n",
    "\n",
    "        if examples[\"answerable\"]:\n",
    "            seq_ids = tokenized_examples.sequence_ids() # marks which tokens come from questions (0), context (1)\n",
    "            offset_mapping = tokenized_examples[\"offset_mapping\"]\n",
    "            conext_token_indices = [i for i, s in enumerate(seq_ids) if s == 1]\n",
    "\n",
    "            if len(conext_token_indices) > 0:\n",
    "                context_start = conext_token_indices[0]\n",
    "                context_end = conext_token_indices[-1]\n",
    "                answer0 = answer_starts\n",
    "                answer1 = answer_starts + len(answers)\n",
    "                i = context_start\n",
    "\n",
    "                # move i forward while the token ends before (or exactly at) the answer start\n",
    "                while i <= context_end and (offset_mapping[i][0] <= answer0 and offset_mapping[i][1] <= answer0):\n",
    "                    i += 1\n",
    "                # step back if we overshot so that offsets[i-1] covers the answer start\n",
    "                while i > context_start and offset_mapping[i-1][0] <= answer0 < offset_mapping[i-1][1]:\n",
    "                    i -= 1\n",
    "\n",
    "                # if i is within the context expand to cover the full answer span\n",
    "                if context_start <= i <= context_end:\n",
    "                    j = i\n",
    "                    while j <= context_end and offset_mapping[j][0] < answer1:\n",
    "                        j += 1\n",
    "                    start = i\n",
    "                    end = min(j-1, context_end) # last token that still overlaps with the answer\n",
    "\n",
    "        tokenized_examples[\"start_positions\"] = start\n",
    "        tokenized_examples[\"end_positions\"] = end\n",
    "        tokenized_examples.pop(\"offset_mapping\", None)\n",
    "\n",
    "        return tokenized_examples\n",
    "    return preprocess_qa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cbe5711-67d5-40a3-88d8-49628ae2cc6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "wk40_bert-base-multilingual-cased/checkpoint-2376\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468c9d93844a4cf18fa8f551ffd984d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2778 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81d01399ceb444f7b0590e53ebbb3fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Augus\\AppData\\Local\\Temp\\ipykernel_22244\\2857210665.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_f1_token 0.4962126068376068\n",
      "eval_precision 1.0\n",
      "eval_recall 1.0\n",
      "\n",
      "\n",
      "wk40_distilbert-base-multilingual-cased/checkpoint-2376\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4385a66bacbe4cf492fd6171e856857e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2778 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fdb510520364a4c9e9114060a60d650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_f1_token 0.546984126984127\n",
      "eval_precision 1.0\n",
      "eval_recall 1.0\n",
      "\n",
      "\n",
      "wk40_xlm-roberta-base/checkpoint-2376\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b4692f35f494fd680ce71354c1d0268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2778 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b43130dc811444d18495a23953a9bd0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_f1_token 0.49696969696969695\n",
      "eval_precision 0.44086021505376344\n",
      "eval_recall 0.5694444444444444\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "print(f\"\")\n",
    "\n",
    "all_model_paths = [\"wk40_bert-base-multilingual-cased/checkpoint-2376\",\n",
    "                   \"wk40_distilbert-base-multilingual-cased/checkpoint-2376\",\n",
    "                   \"wk40_xlm-roberta-base/checkpoint-2376\",\n",
    "]\n",
    "\n",
    "for model_path in all_model_paths:\n",
    "    print()\n",
    "    print()\n",
    "    print(model_path)\n",
    "    print()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    if \"roberta\" in model_path:    \n",
    "        # IO-Token-Labeling\n",
    "        prep = build_preprocess_token(tokenizer)\n",
    "        train_prep = train_ds.map(prep, remove_columns=train_ds.column_names)\n",
    "        test_prep  =  test_ds.map(prep, remove_columns=test_ds.column_names)\n",
    "        \n",
    "        model = AutoModelForTokenClassification.from_pretrained(model_path, num_labels=2)\n",
    "        compute_metrics = compute_metrics_token\n",
    "        data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "    \n",
    "    else:\n",
    "        prep = build_preprocess(tokenizer)\n",
    "        train_prep = train_ds.map(prep, remove_columns=train_ds.column_names)\n",
    "        test_prep  =  test_ds.map(prep, remove_columns=test_ds.column_names)\n",
    "        \n",
    "        model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
    "        compute_metrics = compute_metrics_qa\n",
    "        data_collator = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_prep,\n",
    "        eval_dataset=test_prep,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### Evaluate on the custom questions\n",
    "    metrics = trainer.evaluate(eval_dataset=test_prep)\n",
    "    #print(f\"VAL [{lang}], {metrics}\")\n",
    "    tarr = [(metrics[metric]) for metric in [\"eval_f1_token\", \"eval_precision\", \"eval_recall\"]]\n",
    "    for k,v in zip(tarr, [\"eval_f1_token\", \"eval_precision\", \"eval_recall\"]):\n",
    "        print(v,k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d9a858-bb4c-4c79-9342-925d363b6f33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f33196-0557-4d87-b7d3-29aec79a9990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a259f99-d4d5-4f49-ae1f-d8020668ab1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f27113c-21f7-4c3f-95f2-7a2973b46bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ab1830-6bd8-4239-8b6e-4c0a2595b684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13b21eb-e888-4854-abad-84b0f567468a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42321140-7dba-4bcb-bf54-421e71f826e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576d5359-e186-4619-929f-0abef99b93c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cff6964-3168-4488-a7d3-51223c01c9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
