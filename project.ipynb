{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1af9b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "##IMPORTS\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from collections import Counter\n",
    "from transformers import pipeline\n",
    "from googletrans import Translator\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "525bc9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOWNLOAD DATASET\n",
    "\n",
    "splits = {'train': 'train.parquet', 'validation': 'validation.parquet'}\n",
    "df_train = pd.read_parquet(\"hf://datasets/coastalcph/tydi_xor_rc/\" + splits[\"train\"])\n",
    "df_val = pd.read_parquet(\"hf://datasets/coastalcph/tydi_xor_rc/\" + splits[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a91516c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                question  \\\n",
      "11213         متى تدخلت روسيا في  الحرب الأهلية السورية؟   \n",
      "11214         متى حصلت هنغاريا على استقلالها من النمسا ؟   \n",
      "11215  متى تحالفت فرنسا و بريطانيا العظمى ضد ألمانيا ...   \n",
      "11216   كم عدد ضحايا أول إعتداء إسرائيلي على مدينة غزة ؟   \n",
      "11217       هل سلسلة هاري بوتر مخالفة لقوانين المسيحية ؟   \n",
      "\n",
      "                                                 context lang  answerable  \\\n",
      "11213  The Russian military intervention in the Syria...   ar        True   \n",
      "11214  By 1918, the economic situation had deteriorat...   ar        True   \n",
      "11215  France and Britain declared war on Germany whe...   ar        True   \n",
      "11216  The 2014 Israel–Gaza conflict also known as Op...   ar        True   \n",
      "11217  Religious debates over the \"Harry Potter\" seri...   ar       False   \n",
      "\n",
      "       answer_start                        answer answer_inlang  \n",
      "11213            67                September 2015          None  \n",
      "11214           454                  October 1918          None  \n",
      "11215            81                          1939          None  \n",
      "11216           607  death of thousands of people          None  \n",
      "11217            -1                            no          None  \n",
      "                        question  \\\n",
      "4792           30년 전쟁의 승자는 누구인가?   \n",
      "4793             엑스선은 누가 발견하였는가?   \n",
      "4794  아테네에서 언제 가장 최근의 올림픽이 올렸나요?   \n",
      "4795      세상에서 가장 오래된 방송사는 무엇인가?   \n",
      "4796             팔레스타인 수도는 어딘가요?   \n",
      "\n",
      "                                                context lang  answerable  \\\n",
      "4792  The conflict between France and Spain continue...   ko        True   \n",
      "4793  X-rays make up X-radiation, a form of electrom...   ko        True   \n",
      "4794  In 2022, Beijing will become the first-ever ci...   ko        True   \n",
      "4795  The British Broadcasting Corporation (BBC) is ...   ko        True   \n",
      "4796  Palestine ( '), officially the State of Palest...   ko        True   \n",
      "\n",
      "      answer_start                                  answer answer_inlang  \n",
      "4792            21                                  France          None  \n",
      "4793           503                         Wilhelm Röntgen          None  \n",
      "4794           188                                    2004          None  \n",
      "4795             4  British Broadcasting Corporation (BBC)          None  \n",
      "4796           205                               Jerusalem          None  \n",
      "                                                question  \\\n",
      "13771  ప్రపంచంలో  మొట్టమొదటి దూర విద్య విద్యాలయం ఏ దే...   \n",
      "13772      1959వ సంవత్సరంలో భారతదేశ ప్రధాన మంత్రి  ఎవరు?   \n",
      "13773  ఏ కాకతీయ రాజు కర్నూలు జిల్లాను చివరిగా పాలించాడు?   \n",
      "13774                                మానవ హక్కులు ఎన్ని?   \n",
      "13775       భారదేశంలో అత్యధిక జనాభా కలిగిన రాష్ట్రం ఏది?   \n",
      "\n",
      "                                                 context lang  answerable  \\\n",
      "13771  Referred to as \"People's University\" by Charle...   te        True   \n",
      "13772  Since 1947, there have been 14 different prime...   te        True   \n",
      "13773  Rani Rudrama Devi (died 1289 or 1295), who def...   te        True   \n",
      "13774  The Declaration consists of 30 articles affirm...   te        True   \n",
      "13775  Uttar Pradesh (; IAST: \"Uttar Pradeś\" ) is a s...   te        True   \n",
      "\n",
      "       answer_start            answer answer_inlang  \n",
      "13771           236            London          None  \n",
      "13772           220  Jawaharlal Nehru          None  \n",
      "13773           194      Prataparudra          None  \n",
      "13774            28                30          None  \n",
      "13775             0     Uttar Pradesh          None  \n"
     ]
    }
   ],
   "source": [
    "arabic_records_train = df_train[df_train[\"lang\"] == \"ar\"]\n",
    "print(arabic_records_train.head())\n",
    "\n",
    "\n",
    "korean_records_train = df_train[df_train[\"lang\"] == \"ko\"]\n",
    "print(korean_records_train.head())\n",
    "\n",
    "\n",
    "telugu_records_train = df_train[df_train[\"lang\"] == \"te\"]\n",
    "print(telugu_records_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e57a5c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "#TRANSLATORS           #GPU use if avaiable (device = 0)\n",
    "\n",
    "\n",
    "#Model 'nllb-200-distilled-600M'\n",
    "arabic_translator = pipeline(\n",
    "    task=\"translation\", \n",
    "    model=\"facebook/nllb-200-distilled-600M\",\n",
    "    src_lang=\"arb_Arab\",    \n",
    "    tgt_lang=\"eng_Latn\",   \n",
    "    device=0               \n",
    ")\n",
    "\n",
    "korean_translator = pipeline(\"translation\",\n",
    "                  model=\"facebook/nllb-200-distilled-600M\",\n",
    "                    src_lang=\"kor_Hang\", \n",
    "                    tgt_lang=\"eng_Latn\", \n",
    "                    device=0)\n",
    "\n",
    "telugu_translator = pipeline(\"translation\",\n",
    "                  model=\"facebook/nllb-200-distilled-600M\", \n",
    "                  src_lang=\"tel_Telu\", \n",
    "                  tgt_lang=\"eng_Latn\",\n",
    "                  device=0)\n",
    "\n",
    "#googletrans\n",
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7be7152e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Arabic Records---------\n",
      "Train Size: 2558\n",
      "Val Size: 415\n",
      "Total words in Train: 16320\n",
      "Total words in Val: 2668\n",
      "Top 5 most common words in Arabic training set: [('في', 593), ('من', 587), ('متى', 536), ('ما', 443), ('هو', 350)]\n",
      "في: 593 - Model Translation: In . - Google Translate: in\n",
      "من: 587 - Model Translation: Who ? - Google Translate: from\n",
      "متى: 536 - Model Translation: When ? - Google Translate: when\n",
      "ما: 443 - Model Translation: What ? - Google Translate: what\n",
      "هو: 350 - Model Translation: It 's him . - Google Translate: he\n",
      "---------Korean Records----------\n",
      "Train Size: 2422\n",
      "Val Size: 356\n",
      "Total words in Train: 11863\n",
      "Total words in Val: 1737\n",
      "Top 5 most common words in Korean training set: [('가장', 527), ('무엇인가', 497), ('언제', 336), ('몇', 234), ('어디인가', 228)]\n",
      "가장: 527 - Model Translation: It's the most - Google Translate: most\n",
      "무엇인가: 497 - Model Translation: It's something. - Google Translate: Something\n",
      "언제: 336 - Model Translation: When? - Google Translate: when\n",
      "몇: 234 - Model Translation: A few. - Google Translate: some\n",
      "어디인가: 228 - Model Translation: Where are you? - Google Translate: Where\n",
      "--------Telugu Records-------------\n",
      "Train Size: 1355\n",
      "Val Size: 384\n",
      "Total words in Train: 7668\n",
      "Total words in Val: 2299\n",
      "Top 5 most common words in Telegu training set: [('ఎవరు', 274), ('ఏది', 192), ('ఎన్ని', 165), ('ఎప్పుడు', 154), ('ఏ', 142)]\n",
      "ఎవరు: 274 - Model Translation: Who is it? - Google Translate: Who is\n",
      "ఏది: 192 - Model Translation: What is it? - Google Translate: Which one is\n",
      "ఎన్ని: 165 - Model Translation: How many? - Google Translate: How many\n",
      "ఎప్పుడు: 154 - Model Translation: When? - Google Translate: When\n",
      "ఏ: 142 - Model Translation: There is no - Google Translate: A.\n"
     ]
    }
   ],
   "source": [
    "##WEEK 36\n",
    "\n",
    "## Word level tokenisation using  re '\\W+'\n",
    "#[w for w in re.split(r'\\W+', train_questions) if w] this splits according to \n",
    "#  spaces, punctuation (. , ? !), symbols, etc. And removes empty strings from the list.\n",
    "\n",
    "##!!!!!!!!!!!!!!!!QUESTION: ARE SUPPPOSED TO CONSIDER PONCTUATION AS WORDS????????\n",
    "\n",
    "#Arabic (\"Ar\")\n",
    "print(\"--------Arabic Records---------\")\n",
    "arabic_records_train = df_train[df_train[\"lang\"] == \"ar\"]\n",
    "arabic_records_val = df_val[df_val[\"lang\"] == \"ar\"]\n",
    "print(\"Train Size:\", len(arabic_records_train))\n",
    "print(\"Val Size:\", len(arabic_records_val))\n",
    "\n",
    "\n",
    "train_questions = \" \".join(arabic_records_train[\"question\"].astype(str))\n",
    "val_questions   = \" \".join(arabic_records_val[\"question\"].astype(str))\n",
    "\n",
    "\n",
    "train_words = [w for w in re.split(r'\\W+', train_questions) if w]\n",
    "val_words   = [w for w in re.split(r'\\W+', val_questions) if w]\n",
    "\n",
    "\n",
    "#Word counts\n",
    "total_words_train = len(train_words)\n",
    "total_words_val   = len(val_words)\n",
    "\n",
    "print (\"Total words in Train:\", total_words_train)\n",
    "print (\"Total words in Val:\", total_words_val)\n",
    "\n",
    "\n",
    "\n",
    "#5 most common words in the training set and translations\n",
    "count = Counter(train_words)\n",
    "top5 = count.most_common(5)\n",
    "\n",
    "\n",
    "print(\"Top 5 most common words in Arabic training set:\", top5)\n",
    "\n",
    "for word, freq in top5:\n",
    "        translated_word = arabic_translator(word)[0]['translation_text']\n",
    "\n",
    "        print(f\"{word}: {freq} - Model Translation: {translated_word} - Google Translate: {translator.translate(word, src='ar', dest='en').text}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Korean (\"Ko\")\n",
    "\n",
    "print(\"---------Korean Records----------\")\n",
    "korean_records_train = df_train[df_train[\"lang\"] == \"ko\"]\n",
    "korean_records_val = df_val[df_val[\"lang\"] == \"ko\"]\n",
    "print(\"Train Size:\", len(korean_records_train))\n",
    "print(\"Val Size:\", len(korean_records_val))\n",
    "\n",
    "\n",
    "\n",
    "train_questions = \" \".join(korean_records_train[\"question\"].astype(str))\n",
    "val_questions   = \" \".join(korean_records_val[\"question\"].astype(str))\n",
    "\n",
    "\n",
    "train_words = [w for w in re.split(r'\\W+', train_questions) if w]\n",
    "val_words   = [w for w in re.split(r'\\W+', val_questions) if w]\n",
    "\n",
    "#Word counts\n",
    "total_words_train = len(train_words)\n",
    "total_words_val   = len(val_words)\n",
    "\n",
    "print (\"Total words in Train:\", total_words_train)\n",
    "print (\"Total words in Val:\", total_words_val)\n",
    "\n",
    "\n",
    "\n",
    "#5 most common words in the training set and translations\n",
    "count = Counter(train_words)\n",
    "top5 = count.most_common(5)\n",
    "\n",
    "\n",
    "print(\"Top 5 most common words in Korean training set:\", top5)\n",
    "\n",
    "for word, freq in top5:\n",
    "        translated_word = korean_translator(word)[0]['translation_text']\n",
    "\n",
    "        print(f\"{word}: {freq} - Model Translation: {translated_word} - Google Translate: {translator.translate(word, src='ko', dest='en').text}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Telugu (\"Te\")\n",
    "print(\"--------Telugu Records-------------\")\n",
    "telugu_records_train = df_train[df_train[\"lang\"] == \"te\"]\n",
    "telugu_records_val = df_val[df_val[\"lang\"] == \"te\"]\n",
    "print(\"Train Size:\", len(telugu_records_train))\n",
    "print(\"Val Size:\", len(telugu_records_val))\n",
    "\n",
    "\n",
    "\n",
    "train_questions = \" \".join(telugu_records_train[\"question\"].astype(str))\n",
    "val_questions   = \" \".join(telugu_records_val[\"question\"].astype(str))\n",
    "\n",
    "\n",
    "train_questions = train_questions.replace('?', '')\n",
    "val_questions   = val_questions.replace('?', '')\n",
    "\n",
    "train_words = [w for w in re.split(r'\\s', train_questions) if w]\n",
    "val_words   = [w for w in re.split(r'\\s', val_questions) if w]\n",
    "\n",
    "#Worc counts\n",
    "total_words_train = len(train_words)\n",
    "total_words_val   = len(val_words)\n",
    "\n",
    "print (\"Total words in Train:\", total_words_train)\n",
    "print (\"Total words in Val:\", total_words_val)\n",
    "\n",
    "\n",
    "\n",
    "#5 most common words in the training set and translations\n",
    "count = Counter(train_words)\n",
    "top5 = count.most_common(5)\n",
    "\n",
    "\n",
    "print(\"Top 5 most common words in Telegu training set:\", top5)\n",
    "\n",
    "for word, freq in top5:\n",
    "        translated_word = telugu_translator(word)[0]['translation_text']\n",
    "\n",
    "        print(f\"{word}: {freq} - Model Translation: {translated_word} - Google Translate: {translator.translate(word, src='te', dest='en').text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d92ffc",
   "metadata": {},
   "source": [
    "We conclude that the 5 most common words for each language are \"stop words\" mentioned in the lectures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "273cfe7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        train_acc  val_acc  min_matches_count  min_ratio_threshold\n",
      "Arabic      0.900    0.870                1.0                  0.3\n",
      "Korean      0.972    0.941                1.0                  0.3\n",
      "Telugu      0.979    0.914                1.0                  0.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Rule base classifier\n",
    "#Stop words \n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) | set(string.punctuation)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def translate_question_to_en(question, src_lang):\n",
    "    try:\n",
    "        if src_lang == \"en\" or not question:\n",
    "            return question \n",
    "        translated = translator.translate(question, src=src_lang, dest=\"en\")\n",
    "        return translated.text\n",
    "    except Exception:\n",
    "        return question\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = re.split(r'\\W+', str(text)) \n",
    "\n",
    "    return [\n",
    "        t.lower() \n",
    "        for t in tokens \n",
    "        if  t.lower() not in stop_words\n",
    "    ]\n",
    "\n",
    "\n",
    "#Overlap Score (Substring)\n",
    "#Computes overlap ratio (matched question tokens to the context /total question tokens)\n",
    "#Also retunrs the number of matched tokens. A match is counted if the token is equal or if one contains the other as a substring in either direction\n",
    "def overlap_score_question(question, context,src_lang):\n",
    "    translated_question = translate_question_to_en(question, src_lang)\n",
    "    q_toks = tokenize(translated_question)\n",
    "    c_toks = tokenize(context)\n",
    "    if not q_toks:\n",
    "        return 0.0, 0\n",
    "    \n",
    "    matched = set()\n",
    "    for q in q_toks:\n",
    "        for c in c_toks:\n",
    "            if q == c or q in c or c in q:\n",
    "                matched.add(q)\n",
    "                break\n",
    "    matches = len(matched)\n",
    "    ratio = matches / len(q_toks)\n",
    "    return ratio, matches\n",
    "\n",
    "\n",
    "def tune_parameters(train_df):\n",
    "   \n",
    "    data = [(overlap_score_question(r.question, r.context, r.lang), bool(r.answerable))\n",
    "           for r in train_df.itertuples(index=False)] #storring ratio , number of matches and answerable or not for each question\n",
    "    best = (0, 0, 0.0)  #initializing accuracy, min match count, threshold for the overlap ratio \n",
    "    for k in (1, 2, 3, 4, 5,6,7,8,9,10):\n",
    "        for threshold in (0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9):\n",
    "            correct = 0\n",
    "            for (ratio, match_count), y in data:\n",
    "                pred = (match_count >= k) and (ratio >= threshold)\n",
    "                correct += int(pred == y)\n",
    "            acc = correct / len(data) if data else 0\n",
    "            if acc > best[0]:\n",
    "                best = (acc, k, threshold)\n",
    "    return {\"min_match_count\": best[1], \"min_ratio_threshold\": best[2], \"best_train_acc\": best[0]}\n",
    "\n",
    "\n",
    "def eval(df, min_matches, ratio_threshold):\n",
    "    correct = 0\n",
    "    for r in df.itertuples(index=False):\n",
    "        ratio, m = overlap_score_question(r.question, r.context, r.lang)\n",
    "        pred = (m >= min_matches) and (ratio >= ratio_threshold)\n",
    "        correct += int(pred == bool(r.answerable))\n",
    "    return correct / len(df) if len(df) else 0.0\n",
    "\n",
    "\n",
    "\n",
    "def run_rule_classifier(df_train, df_val):\n",
    "  \n",
    "    results = {}\n",
    "\n",
    "    for code, name in [(\"ar\",\"Arabic\"), (\"ko\",\"Korean\"), (\"te\",\"Telugu\")]:\n",
    "        train = df_train[df_train.lang == code]\n",
    "        val = df_val[df_val.lang == code]\n",
    "        params = tune_parameters(train)\n",
    "        val_acc = eval(val, params[\"min_match_count\"], params[\"min_ratio_threshold\"])\n",
    "        results[name] = {\n",
    "            \"train_acc\": round(params[\"best_train_acc\"], 3),\n",
    "            \"val_acc\": round(val_acc, 3),\n",
    "            \"min_matches_count\": params[\"min_match_count\"],\n",
    "            \"min_ratio_threshold\": params[\"min_ratio_threshold\"]\n",
    "        }\n",
    "    return pd.DataFrame(results).T\n",
    "\n",
    "\n",
    "results = run_rule_classifier(df_train, df_val)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46273798",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WEEK 37\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
